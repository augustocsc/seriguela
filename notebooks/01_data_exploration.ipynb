{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cce831300b4805b3ab98118d9f725c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing rows that didn't work with sympy\n",
      "Replacing the constants with letter C\n",
      "Renaming the column\n",
      "Creating a column with prefix expression\n",
      "Removing rows that didn't work with sympy\n",
      "Replacing the constants with letter C\n",
      "Renaming the column\n",
      "Creating a column with prefix expression\n",
      "Removing rows that didn't work with sympy\n",
      "Replacing the constants with letter C\n",
      "Renaming the column\n",
      "Creating a column with prefix expression\n",
      "Removing rows that didn't work with sympy\n",
      "Replacing the constants with letter C\n",
      "Renaming the column\n",
      "Creating a column with prefix expression\n",
      "Removing rows that didn't work with sympy\n",
      "Replacing the constants with letter C\n",
      "Renaming the column\n",
      "Creating a column with prefix expression\n",
      "Removing rows that didn't work with sympy\n",
      "Replacing the constants with letter C\n",
      "Renaming the column\n",
      "Creating a column with prefix expression\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "scripts_path = os.path.abspath(os.path.join(\"..\", \"scripts\"))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "scripts_path = os.path.abspath(os.path.join(\"..\", \"classes\"))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sympy import simplify, sympify\n",
    "import data.data_cleaning as dc\n",
    "from expression import Expression\n",
    "#Load file data/raw/13k.csv\n",
    "\n",
    "\n",
    "def apply_chunk(chunk, func):\n",
    "    \"\"\"Helper function to apply a function to a chunk of data.\"\"\"\n",
    "    return chunk.apply(func)\n",
    "\n",
    "def parallel_apply(series, func, n_jobs=None):\n",
    "    \"\"\"Apply a function to a pandas Series in parallel.\"\"\"\n",
    "    n_jobs = mp.cpu_count() if n_jobs is None else n_jobs\n",
    "    # Split into roughly equal chunks\n",
    "    chunks = np.array_split(series, n_jobs)\n",
    "    with mp.Pool(n_jobs) as pool:\n",
    "        # Use the helper function instead of a lambda\n",
    "        results = pool.starmap(apply_chunk, [(chunk, func) for chunk in chunks])\n",
    "    # Concatenate the resulting Series\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Load file data/raw/100k.csv\n",
    "file_path = '../data/raw/500k.csv'\n",
    "chunk_size = 100000  # Define the chunk size\n",
    "processed_chunks = []\n",
    "\n",
    "# Initialize the progress bar\n",
    "total_rows = sum(1 for _ in open(file_path)) - 1  # Total rows (excluding header)\n",
    "total_chunks = (total_rows // chunk_size) + 1  # Total number of chunks\n",
    "with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
    "    # Read the file in chunks and process each chunk\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Keeping only the equation column\n",
    "        chunk = chunk[['eq']]\n",
    "        # Removing rows that didn't work with sympy\n",
    "        print(\"Removing rows that didn't work with sympy\")\n",
    "        chunk = chunk[~chunk['eq'].str.contains('ERROR_simplify')]\n",
    "        # Replacing the constants with letter C using parallel_apply\n",
    "        print(\"Replacing the constants with letter C\")\n",
    "        chunk['eq'] = parallel_apply(chunk['eq'], dc.augment_expression)\n",
    "        # Renaming the column\n",
    "        print(\"Renaming the column\")\n",
    "        chunk.rename(columns={'eq': 'infix_expr'}, inplace=True)\n",
    "        \n",
    "        # Create a column with prefix expression using parallel_apply\n",
    "        print(\"Creating a column with prefix expression\")\n",
    "        chunk['prefix_expr'] = parallel_apply(chunk['infix_expr'], Expression.infix_to_prefix)\n",
    "        processed_chunks.append(chunk)\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Combine all processed chunks into a single DataFrame\n",
    "temp_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# remove duplicates\n",
    "temp_df = temp_df.drop_duplicates(subset=['infix_expr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.parallel_utils import augment_dataframe_parallel\n",
    "\n",
    "df_augmented = augment_dataframe_parallel(temp_df, expression_col=\"infix_expr\", n_jobs=4)\n",
    "df_augmented.rename(columns={'simple': 'i_simple'}, inplace=True)\n",
    "df_augmented.rename(columns={'key_value': 'i_key_value'}, inplace=True)\n",
    "df_augmented.rename(columns={'delimiter': 'i_delimiter'}, inplace=True)\n",
    "df_augmented.rename(columns={'minimalist': 'i_minimalist'}, inplace=True)\n",
    "\n",
    "\n",
    "df_augmented = augment_dataframe_parallel(df_augmented, expression_col=\"prefix_expr\", n_jobs=4)\n",
    "df_augmented.rename(columns={'simple': 'p_simple'}, inplace=True)\n",
    "df_augmented.rename(columns={'key_value': 'p_key_value'}, inplace=True)\n",
    "df_augmented.rename(columns={'delimiter': 'p_delimiter'}, inplace=True)\n",
    "df_augmented.rename(columns={'minimalist': 'p_minimalist'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_augmented into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df_augmented, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "file = os.path.basename(file_path)  # Extract the file name from file_path\n",
    "train_file_path = f'../data/processed/{file.replace(\".csv\", \"\")}/train_{file}'\n",
    "val_file_path = f'../data/processed/{file.replace(\".csv\", \"\")}/val_{file}'\n",
    "test_file_path = f'../data/processed/{file.replace(\".csv\", \"\")}/test_{file}'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(train_file_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(val_file_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(test_file_path), exist_ok=True)\n",
    "\n",
    "# Save the train, validation, and test sets\n",
    "train_df.to_csv(train_file_path, index=False)\n",
    "val_df.to_csv(val_file_path, index=False)\n",
    "test_df.to_csv(test_file_path, index=False)\n",
    "\n",
    "# Save the processed file\n",
    "processed_file_path = f'../data/processed/{file.replace(\".csv\", \"\")}/{file}'\n",
    "temp_df.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/augustocsc/sintetico/commit/2dd4a439794703e40ec85cf995ed25369086305a', commit_message='Upload folder using huggingface_hub', commit_description='', oid='2dd4a439794703e40ec85cf995ed25369086305a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/augustocsc/sintetico', endpoint='https://huggingface.co', repo_type='dataset', repo_id='augustocsc/sintetico'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "folder = \"10k\"\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=f\"../data/processed/{folder}\",\n",
    "    repo_id=\"augustocsc/sintetico\",\n",
    "    repo_type=\"dataset\",\n",
    "    path_in_repo= folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Ensure you are logged in to Hugging Face\n",
    "# Login using `huggingface-cli login` in the terminal if not already logged in\n",
    "ds = load_dataset(\"augustocsc/sintetico\", data_dir=\"10k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cb4670d1a54457a77baf3d6d28aeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer from adapter repo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"augustocsc/Se124M10K\")\n",
    "\n",
    "# Load base GPT2 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resize embeddings to match tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load the LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, \"augustocsc/Se124M10K\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Generate a mathematical expression using variables ['x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8'] and operands ['*', '+', '-', 'asin', 'cos', 'pow', 'sin', 'tan'] and ['C'] as constant.\n",
      "Expression: sin(x_7**C) - C * x_6*x_7 + C * x_1 - C\n",
      "\n",
      "Expression: asin(exp(x_1))*x_3 - cos((x_1 -\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Instruction: Generate a mathematical expression using variables ['x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8'] and operands ['*', '+', '-', 'asin', 'cos', 'pow', 'sin', 'tan'] and ['C'] as constant.\\nExpression: <|startofex|>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C*x_1 - C*x_6*x_7 - C + sin(x_7**C)\n"
     ]
    }
   ],
   "source": [
    "from sympy import sympify\n",
    "\n",
    "# Example: Load an expression from a string\n",
    "expression_str = \" sin(x_7**C) - C * x_6*x_7 + C * x_1 - C\"\n",
    "expression = sympify(expression_str)\n",
    "print(expression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".seriguela",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
