  0%|                                                          | 0/220500 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  1%|▍                                            | 2203/220500 [02:59<3:33:13, 17.06it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.6213, 'grad_norm': 0.23669062554836273, 'learning_rate': 4.997755102040816e-05, 'epoch': 0.05}
{'loss': 0.3757, 'grad_norm': 0.31748318672180176, 'learning_rate': 4.995487528344672e-05, 'epoch': 0.09}
{'loss': 0.245, 'grad_norm': 0.3073875308036804, 'learning_rate': 4.993219954648526e-05, 'epoch': 0.14}
{'loss': 0.2067, 'grad_norm': 0.34453877806663513, 'learning_rate': 4.990952380952381e-05, 'epoch': 0.18}
{'loss': 0.1887, 'grad_norm': 0.32513177394866943, 'learning_rate': 4.988684807256236e-05, 'epoch': 0.23}
{'loss': 0.1788, 'grad_norm': 0.31589552760124207, 'learning_rate': 4.986417233560091e-05, 'epoch': 0.27}
{'loss': 0.1693, 'grad_norm': 0.31726202368736267, 'learning_rate': 4.984149659863946e-05, 'epoch': 0.32}
{'loss': 0.1659, 'grad_norm': 0.3459065854549408, 'learning_rate': 4.9818820861678004e-05, 'epoch': 0.36}
{'loss': 0.1628, 'grad_norm': 0.32710057497024536, 'learning_rate': 4.9796145124716554e-05, 'epoch': 0.41}
{'loss': 0.1575, 'grad_norm': 0.30756402015686035, 'learning_rate': 4.9773469387755104e-05, 'epoch': 0.45}
{'loss': 0.1595, 'grad_norm': 0.3379279375076294, 'learning_rate': 4.9750793650793654e-05, 'epoch': 0.5}
{'loss': 0.1511, 'grad_norm': 0.3255889117717743, 'learning_rate': 4.9728117913832204e-05, 'epoch': 0.54}
{'loss': 0.1532, 'grad_norm': 0.32930490374565125, 'learning_rate': 4.9705442176870754e-05, 'epoch': 0.59}
{'loss': 0.1512, 'grad_norm': 0.25651273131370544, 'learning_rate': 4.9682766439909304e-05, 'epoch': 0.63}
{'loss': 0.151, 'grad_norm': 0.31016722321510315, 'learning_rate': 4.966009070294785e-05, 'epoch': 0.68}
{'loss': 0.1486, 'grad_norm': 0.28214409947395325, 'learning_rate': 4.96374149659864e-05, 'epoch': 0.73}
{'loss': 0.1452, 'grad_norm': 0.3092798888683319, 'learning_rate': 4.961473922902495e-05, 'epoch': 0.77}
{'loss': 0.1487, 'grad_norm': 0.2994401752948761, 'learning_rate': 4.95920634920635e-05, 'epoch': 0.82}
{'loss': 0.1463, 'grad_norm': 0.32052183151245117, 'learning_rate': 4.956938775510204e-05, 'epoch': 0.86}
{'loss': 0.1446, 'grad_norm': 0.3364502191543579, 'learning_rate': 4.954671201814059e-05, 'epoch': 0.91}
{'loss': 0.1411, 'grad_norm': 0.2696020305156708, 'learning_rate': 4.952403628117914e-05, 'epoch': 0.95}
{'loss': 0.1439, 'grad_norm': 0.2520163357257843, 'learning_rate': 4.950136054421769e-05, 'epoch': 1.0}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  1%|▍                                            | 2205/220500 [04:37<3:33:13, 17.06it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.542216420173645, 'eval_runtime': 97.718, 'eval_samples_per_second': 155.028, 'eval_steps_per_second': 4.851, 'epoch': 1.0}
  2%|▉                                            | 4408/220500 [06:47<3:28:14, 17.29it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1453, 'grad_norm': 0.30646130442619324, 'learning_rate': 4.947868480725624e-05, 'epoch': 1.04}
{'loss': 0.1433, 'grad_norm': 0.2969256043434143, 'learning_rate': 4.945600907029478e-05, 'epoch': 1.09}
{'loss': 0.1415, 'grad_norm': 0.2872678339481354, 'learning_rate': 4.943333333333334e-05, 'epoch': 1.13}
{'loss': 0.1408, 'grad_norm': 0.3337783217430115, 'learning_rate': 4.941065759637188e-05, 'epoch': 1.18}
{'loss': 0.1431, 'grad_norm': 0.2242783010005951, 'learning_rate': 4.938798185941043e-05, 'epoch': 1.22}
{'loss': 0.14, 'grad_norm': 0.32639822363853455, 'learning_rate': 4.936530612244898e-05, 'epoch': 1.27}
{'loss': 0.1412, 'grad_norm': 0.30973491072654724, 'learning_rate': 4.934263038548753e-05, 'epoch': 1.32}
{'loss': 0.1424, 'grad_norm': 0.2533072829246521, 'learning_rate': 4.9319954648526076e-05, 'epoch': 1.36}
{'loss': 0.1384, 'grad_norm': 0.273505300283432, 'learning_rate': 4.9297278911564626e-05, 'epoch': 1.41}
{'loss': 0.1414, 'grad_norm': 0.3055315613746643, 'learning_rate': 4.9274603174603176e-05, 'epoch': 1.45}
{'loss': 0.1368, 'grad_norm': 0.250862717628479, 'learning_rate': 4.9251927437641726e-05, 'epoch': 1.5}
{'loss': 0.1399, 'grad_norm': 0.3139622211456299, 'learning_rate': 4.9229251700680276e-05, 'epoch': 1.54}
{'loss': 0.1405, 'grad_norm': 0.3321200907230377, 'learning_rate': 4.920657596371882e-05, 'epoch': 1.59}
{'loss': 0.1391, 'grad_norm': 0.26714518666267395, 'learning_rate': 4.9183900226757376e-05, 'epoch': 1.63}
{'loss': 0.1383, 'grad_norm': 0.2645431458950043, 'learning_rate': 4.916122448979592e-05, 'epoch': 1.68}
{'loss': 0.1371, 'grad_norm': 0.28354519605636597, 'learning_rate': 4.913854875283447e-05, 'epoch': 1.72}
{'loss': 0.1357, 'grad_norm': 0.2715573012828827, 'learning_rate': 4.911587301587302e-05, 'epoch': 1.77}
{'loss': 0.1357, 'grad_norm': 0.2539803981781006, 'learning_rate': 4.909319727891157e-05, 'epoch': 1.81}
{'loss': 0.1366, 'grad_norm': 0.2510261535644531, 'learning_rate': 4.907052154195011e-05, 'epoch': 1.86}
{'loss': 0.138, 'grad_norm': 0.2979539632797241, 'learning_rate': 4.904784580498866e-05, 'epoch': 1.9}
{'loss': 0.1377, 'grad_norm': 0.3399156928062439, 'learning_rate': 4.902517006802721e-05, 'epoch': 1.95}
{'loss': 0.1356, 'grad_norm': 0.251825213432312, 'learning_rate': 4.900249433106576e-05, 'epoch': 2.0}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  2%|▉                                            | 4410/220500 [08:21<3:28:14, 17.29it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.5177043080329895, 'eval_runtime': 93.4579, 'eval_samples_per_second': 162.094, 'eval_steps_per_second': 5.072, 'epoch': 2.0}
  3%|█▎                                           | 6613/220500 [10:31<3:26:44, 17.24it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1365, 'grad_norm': 0.318107545375824, 'learning_rate': 4.897981859410431e-05, 'epoch': 2.04}
{'loss': 0.1353, 'grad_norm': 0.27037176489830017, 'learning_rate': 4.8957142857142855e-05, 'epoch': 2.09}
{'loss': 0.1335, 'grad_norm': 0.27617090940475464, 'learning_rate': 4.893446712018141e-05, 'epoch': 2.13}
{'loss': 0.1357, 'grad_norm': 0.2300763577222824, 'learning_rate': 4.8911791383219955e-05, 'epoch': 2.18}
{'loss': 0.1343, 'grad_norm': 0.27473124861717224, 'learning_rate': 4.8889115646258505e-05, 'epoch': 2.22}
{'loss': 0.1352, 'grad_norm': 0.23615221679210663, 'learning_rate': 4.8866439909297055e-05, 'epoch': 2.27}
{'loss': 0.1338, 'grad_norm': 0.2693379521369934, 'learning_rate': 4.8843764172335605e-05, 'epoch': 2.31}
{'loss': 0.1335, 'grad_norm': 0.2653670907020569, 'learning_rate': 4.882108843537415e-05, 'epoch': 2.36}
{'loss': 0.1338, 'grad_norm': 0.27738815546035767, 'learning_rate': 4.87984126984127e-05, 'epoch': 2.4}
{'loss': 0.1336, 'grad_norm': 0.24119549989700317, 'learning_rate': 4.877573696145125e-05, 'epoch': 2.45}
{'loss': 0.1339, 'grad_norm': 0.26131823658943176, 'learning_rate': 4.87530612244898e-05, 'epoch': 2.49}
{'loss': 0.1334, 'grad_norm': 0.25553932785987854, 'learning_rate': 4.873038548752835e-05, 'epoch': 2.54}
{'loss': 0.1325, 'grad_norm': 0.267316609621048, 'learning_rate': 4.870770975056689e-05, 'epoch': 2.59}
{'loss': 0.133, 'grad_norm': 0.27024713158607483, 'learning_rate': 4.868503401360545e-05, 'epoch': 2.63}
{'loss': 0.134, 'grad_norm': 0.24764926731586456, 'learning_rate': 4.866235827664399e-05, 'epoch': 2.68}
{'loss': 0.132, 'grad_norm': 0.2512240409851074, 'learning_rate': 4.863968253968254e-05, 'epoch': 2.72}
{'loss': 0.133, 'grad_norm': 0.22248485684394836, 'learning_rate': 4.861700680272109e-05, 'epoch': 2.77}
{'loss': 0.1317, 'grad_norm': 0.22575527429580688, 'learning_rate': 4.859433106575964e-05, 'epoch': 2.81}
{'loss': 0.1319, 'grad_norm': 0.293986976146698, 'learning_rate': 4.8571655328798185e-05, 'epoch': 2.86}
{'loss': 0.1346, 'grad_norm': 0.23116490244865417, 'learning_rate': 4.8548979591836735e-05, 'epoch': 2.9}
{'loss': 0.1339, 'grad_norm': 0.32721981406211853, 'learning_rate': 4.852630385487529e-05, 'epoch': 2.95}
{'loss': 0.1316, 'grad_norm': 0.2526114881038666, 'learning_rate': 4.8503628117913835e-05, 'epoch': 2.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  3%|█▎                                           | 6615/220500 [12:04<3:26:44, 17.24it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.5084888935089111, 'eval_runtime': 92.9686, 'eval_samples_per_second': 162.947, 'eval_steps_per_second': 5.098, 'epoch': 3.0}
  4%|█▊                                           | 8818/220500 [14:14<3:24:43, 17.23it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1308, 'grad_norm': 0.2453877031803131, 'learning_rate': 4.8480952380952385e-05, 'epoch': 3.04}
{'loss': 0.1326, 'grad_norm': 0.2759704887866974, 'learning_rate': 4.845827664399093e-05, 'epoch': 3.08}
{'loss': 0.1323, 'grad_norm': 0.19868966937065125, 'learning_rate': 4.8435600907029484e-05, 'epoch': 3.13}
{'loss': 0.1301, 'grad_norm': 0.29832205176353455, 'learning_rate': 4.841292517006803e-05, 'epoch': 3.17}
{'loss': 0.132, 'grad_norm': 0.2268105298280716, 'learning_rate': 4.839024943310658e-05, 'epoch': 3.22}
{'loss': 0.1315, 'grad_norm': 0.2595742642879486, 'learning_rate': 4.836757369614513e-05, 'epoch': 3.27}
{'loss': 0.1309, 'grad_norm': 0.2104065716266632, 'learning_rate': 4.834489795918368e-05, 'epoch': 3.31}
{'loss': 0.1307, 'grad_norm': 0.24501287937164307, 'learning_rate': 4.832222222222223e-05, 'epoch': 3.36}
{'loss': 0.1306, 'grad_norm': 0.27119892835617065, 'learning_rate': 4.829954648526077e-05, 'epoch': 3.4}
{'loss': 0.1295, 'grad_norm': 0.21231918036937714, 'learning_rate': 4.827687074829933e-05, 'epoch': 3.45}
{'loss': 0.1312, 'grad_norm': 0.2913980782032013, 'learning_rate': 4.825419501133787e-05, 'epoch': 3.49}
{'loss': 0.1318, 'grad_norm': 0.24941395223140717, 'learning_rate': 4.823151927437642e-05, 'epoch': 3.54}
{'loss': 0.1304, 'grad_norm': 0.28548240661621094, 'learning_rate': 4.8208843537414964e-05, 'epoch': 3.58}
{'loss': 0.1308, 'grad_norm': 0.22970044612884521, 'learning_rate': 4.818616780045352e-05, 'epoch': 3.63}
{'loss': 0.128, 'grad_norm': 0.2354571670293808, 'learning_rate': 4.8163492063492064e-05, 'epoch': 3.67}
{'loss': 0.1281, 'grad_norm': 0.19360129535198212, 'learning_rate': 4.8140816326530614e-05, 'epoch': 3.72}
{'loss': 0.131, 'grad_norm': 0.2580355405807495, 'learning_rate': 4.8118140589569164e-05, 'epoch': 3.76}
{'loss': 0.129, 'grad_norm': 0.2693229913711548, 'learning_rate': 4.8095464852607714e-05, 'epoch': 3.81}
{'loss': 0.1302, 'grad_norm': 0.28157106041908264, 'learning_rate': 4.8072789115646264e-05, 'epoch': 3.85}
{'loss': 0.1325, 'grad_norm': 0.2918575704097748, 'learning_rate': 4.805011337868481e-05, 'epoch': 3.9}
{'loss': 0.1294, 'grad_norm': 0.21532824635505676, 'learning_rate': 4.8027437641723364e-05, 'epoch': 3.95}
{'loss': 0.1296, 'grad_norm': 0.21170851588249207, 'learning_rate': 4.800476190476191e-05, 'epoch': 3.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  4%|█▊                                           | 8820/220500 [15:48<3:24:43, 17.23it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.5010705590248108, 'eval_runtime': 93.8384, 'eval_samples_per_second': 161.437, 'eval_steps_per_second': 5.051, 'epoch': 4.0}
  5%|██▏                                         | 11023/220500 [18:00<3:25:38, 16.98it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1308, 'grad_norm': 0.21529652178287506, 'learning_rate': 4.798208616780046e-05, 'epoch': 4.04}
{'loss': 0.1301, 'grad_norm': 0.22308297455310822, 'learning_rate': 4.7959410430839e-05, 'epoch': 4.08}
{'loss': 0.129, 'grad_norm': 0.27445095777511597, 'learning_rate': 4.793673469387756e-05, 'epoch': 4.13}
{'loss': 0.1298, 'grad_norm': 0.21346484124660492, 'learning_rate': 4.79140589569161e-05, 'epoch': 4.17}
{'loss': 0.1293, 'grad_norm': 0.23609264194965363, 'learning_rate': 4.789138321995465e-05, 'epoch': 4.22}
{'loss': 0.1285, 'grad_norm': 0.22090595960617065, 'learning_rate': 4.78687074829932e-05, 'epoch': 4.26}
{'loss': 0.1288, 'grad_norm': 0.22501105070114136, 'learning_rate': 4.784603174603175e-05, 'epoch': 4.31}
{'loss': 0.1269, 'grad_norm': 0.22487378120422363, 'learning_rate': 4.78233560090703e-05, 'epoch': 4.35}
{'loss': 0.1317, 'grad_norm': 0.25094982981681824, 'learning_rate': 4.780068027210884e-05, 'epoch': 4.4}
{'loss': 0.127, 'grad_norm': 0.21027158200740814, 'learning_rate': 4.77780045351474e-05, 'epoch': 4.44}
{'loss': 0.1288, 'grad_norm': 0.2192791998386383, 'learning_rate': 4.775532879818594e-05, 'epoch': 4.49}
{'loss': 0.1302, 'grad_norm': 0.23291540145874023, 'learning_rate': 4.773265306122449e-05, 'epoch': 4.54}
{'loss': 0.1279, 'grad_norm': 0.24361081421375275, 'learning_rate': 4.7709977324263036e-05, 'epoch': 4.58}
{'loss': 0.1305, 'grad_norm': 0.23645102977752686, 'learning_rate': 4.768730158730159e-05, 'epoch': 4.63}
{'loss': 0.1315, 'grad_norm': 0.24964824318885803, 'learning_rate': 4.7664625850340136e-05, 'epoch': 4.67}
{'loss': 0.1308, 'grad_norm': 0.21026450395584106, 'learning_rate': 4.7641950113378686e-05, 'epoch': 4.72}
{'loss': 0.1299, 'grad_norm': 0.24376000463962555, 'learning_rate': 4.7619274376417236e-05, 'epoch': 4.76}
{'loss': 0.1263, 'grad_norm': 0.2337179183959961, 'learning_rate': 4.7596598639455786e-05, 'epoch': 4.81}
{'loss': 0.1293, 'grad_norm': 0.32991522550582886, 'learning_rate': 4.7573922902494336e-05, 'epoch': 4.85}
{'loss': 0.1281, 'grad_norm': 0.19686287641525269, 'learning_rate': 4.755124716553288e-05, 'epoch': 4.9}
{'loss': 0.127, 'grad_norm': 0.22939209640026093, 'learning_rate': 4.7528571428571436e-05, 'epoch': 4.94}
{'loss': 0.1299, 'grad_norm': 0.25918540358543396, 'learning_rate': 4.750589569160998e-05, 'epoch': 4.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  5%|██▏                                         | 11025/220500 [19:36<3:25:38, 16.98it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4953380227088928, 'eval_runtime': 95.9707, 'eval_samples_per_second': 157.85, 'eval_steps_per_second': 4.939, 'epoch': 5.0}
  6%|██▋                                         | 13228/220500 [21:52<3:29:56, 16.45it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1286, 'grad_norm': 0.20288987457752228, 'learning_rate': 4.748321995464853e-05, 'epoch': 5.03}
{'loss': 0.1293, 'grad_norm': 0.23693276941776276, 'learning_rate': 4.746054421768707e-05, 'epoch': 5.08}
{'loss': 0.1262, 'grad_norm': 0.19865286350250244, 'learning_rate': 4.743786848072563e-05, 'epoch': 5.12}
{'loss': 0.1294, 'grad_norm': 0.20010046660900116, 'learning_rate': 4.741519274376417e-05, 'epoch': 5.17}
{'loss': 0.1273, 'grad_norm': 0.22961515188217163, 'learning_rate': 4.739251700680272e-05, 'epoch': 5.22}
{'loss': 0.127, 'grad_norm': 0.189922034740448, 'learning_rate': 4.736984126984127e-05, 'epoch': 5.26}
{'loss': 0.1267, 'grad_norm': 0.21453511714935303, 'learning_rate': 4.734716553287982e-05, 'epoch': 5.31}
{'loss': 0.1244, 'grad_norm': 0.21269546449184418, 'learning_rate': 4.732448979591837e-05, 'epoch': 5.35}
{'loss': 0.1259, 'grad_norm': 0.19017396867275238, 'learning_rate': 4.7301814058956915e-05, 'epoch': 5.4}
{'loss': 0.1273, 'grad_norm': 0.1841483861207962, 'learning_rate': 4.727913832199547e-05, 'epoch': 5.44}
{'loss': 0.1267, 'grad_norm': 0.22482651472091675, 'learning_rate': 4.7256462585034015e-05, 'epoch': 5.49}
{'loss': 0.1274, 'grad_norm': 0.17716184258460999, 'learning_rate': 4.7233786848072565e-05, 'epoch': 5.53}
{'loss': 0.1291, 'grad_norm': 0.21409906446933746, 'learning_rate': 4.721111111111111e-05, 'epoch': 5.58}
{'loss': 0.1276, 'grad_norm': 0.19527983665466309, 'learning_rate': 4.7188435374149665e-05, 'epoch': 5.62}
{'loss': 0.1285, 'grad_norm': 0.1950533539056778, 'learning_rate': 4.7165759637188215e-05, 'epoch': 5.67}
{'loss': 0.1263, 'grad_norm': 0.29488012194633484, 'learning_rate': 4.714308390022676e-05, 'epoch': 5.71}
{'loss': 0.1276, 'grad_norm': 0.21912118792533875, 'learning_rate': 4.712040816326531e-05, 'epoch': 5.76}
{'loss': 0.1276, 'grad_norm': 0.17120246589183807, 'learning_rate': 4.709773242630386e-05, 'epoch': 5.8}
{'loss': 0.1263, 'grad_norm': 0.2264024168252945, 'learning_rate': 4.707505668934241e-05, 'epoch': 5.85}
{'loss': 0.1283, 'grad_norm': 0.23451243340969086, 'learning_rate': 4.705238095238095e-05, 'epoch': 5.9}
{'loss': 0.1288, 'grad_norm': 0.2236257791519165, 'learning_rate': 4.702970521541951e-05, 'epoch': 5.94}
{'loss': 0.1301, 'grad_norm': 0.1927127242088318, 'learning_rate': 4.700702947845805e-05, 'epoch': 5.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  6%|██▋                                         | 13230/220500 [23:28<3:29:56, 16.45it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.49161040782928467, 'eval_runtime': 95.4021, 'eval_samples_per_second': 158.791, 'eval_steps_per_second': 4.968, 'epoch': 6.0}
  7%|███                                         | 15433/220500 [25:40<3:26:09, 16.58it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1291, 'grad_norm': 0.20963835716247559, 'learning_rate': 4.69843537414966e-05, 'epoch': 6.03}
{'loss': 0.1284, 'grad_norm': 0.2848558723926544, 'learning_rate': 4.696167800453515e-05, 'epoch': 6.08}
{'loss': 0.1289, 'grad_norm': 0.27510935068130493, 'learning_rate': 4.69390022675737e-05, 'epoch': 6.12}
{'loss': 0.1271, 'grad_norm': 0.20344246923923492, 'learning_rate': 4.691632653061225e-05, 'epoch': 6.17}
{'loss': 0.1279, 'grad_norm': 0.24023012816905975, 'learning_rate': 4.6893650793650794e-05, 'epoch': 6.21}
{'loss': 0.1275, 'grad_norm': 0.19404135644435883, 'learning_rate': 4.6870975056689344e-05, 'epoch': 6.26}
{'loss': 0.1259, 'grad_norm': 0.2113020122051239, 'learning_rate': 4.6848299319727894e-05, 'epoch': 6.3}
{'loss': 0.1256, 'grad_norm': 0.21489500999450684, 'learning_rate': 4.6825623582766444e-05, 'epoch': 6.35}
{'loss': 0.1281, 'grad_norm': 0.19010938704013824, 'learning_rate': 4.680294784580499e-05, 'epoch': 6.39}
{'loss': 0.1241, 'grad_norm': 0.19464755058288574, 'learning_rate': 4.678027210884354e-05, 'epoch': 6.44}
{'loss': 0.1283, 'grad_norm': 0.21018201112747192, 'learning_rate': 4.675759637188209e-05, 'epoch': 6.49}
{'loss': 0.1262, 'grad_norm': 0.23685944080352783, 'learning_rate': 4.673492063492064e-05, 'epoch': 6.53}
{'loss': 0.1275, 'grad_norm': 0.19442622363567352, 'learning_rate': 4.671224489795919e-05, 'epoch': 6.58}
{'loss': 0.1269, 'grad_norm': 0.27254948019981384, 'learning_rate': 4.668956916099774e-05, 'epoch': 6.62}
{'loss': 0.1278, 'grad_norm': 0.21336813271045685, 'learning_rate': 4.666689342403629e-05, 'epoch': 6.67}
{'loss': 0.1265, 'grad_norm': 0.26835897564888, 'learning_rate': 4.664421768707483e-05, 'epoch': 6.71}
{'loss': 0.1267, 'grad_norm': 0.21998600661754608, 'learning_rate': 4.662154195011338e-05, 'epoch': 6.76}
{'loss': 0.127, 'grad_norm': 0.1935921013355255, 'learning_rate': 4.659886621315193e-05, 'epoch': 6.8}
{'loss': 0.1254, 'grad_norm': 0.18474417924880981, 'learning_rate': 4.657619047619048e-05, 'epoch': 6.85}
{'loss': 0.1278, 'grad_norm': 0.25289246439933777, 'learning_rate': 4.6553514739229023e-05, 'epoch': 6.89}
{'loss': 0.1275, 'grad_norm': 0.20510415732860565, 'learning_rate': 4.6530839002267573e-05, 'epoch': 6.94}
{'loss': 0.1257, 'grad_norm': 0.19266648590564728, 'learning_rate': 4.650816326530612e-05, 'epoch': 6.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  7%|███                                         | 15435/220500 [27:14<3:26:09, 16.58it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.48736053705215454, 'eval_runtime': 94.5095, 'eval_samples_per_second': 160.291, 'eval_steps_per_second': 5.015, 'epoch': 7.0}
  8%|███▌                                        | 17640/220500 [29:30<3:13:53, 17.44it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1238, 'grad_norm': 0.19423344731330872, 'learning_rate': 4.648548752834467e-05, 'epoch': 7.03}
{'loss': 0.1251, 'grad_norm': 0.18020294606685638, 'learning_rate': 4.646281179138322e-05, 'epoch': 7.07}
{'loss': 0.1277, 'grad_norm': 0.18721133470535278, 'learning_rate': 4.644013605442177e-05, 'epoch': 7.12}
{'loss': 0.1236, 'grad_norm': 0.2286110669374466, 'learning_rate': 4.641746031746032e-05, 'epoch': 7.17}
{'loss': 0.1251, 'grad_norm': 0.18357612192630768, 'learning_rate': 4.6394784580498866e-05, 'epoch': 7.21}
{'loss': 0.1268, 'grad_norm': 0.22107049822807312, 'learning_rate': 4.6372108843537416e-05, 'epoch': 7.26}
{'loss': 0.125, 'grad_norm': 0.19048087298870087, 'learning_rate': 4.6349433106575966e-05, 'epoch': 7.3}
{'loss': 0.1257, 'grad_norm': 0.17498448491096497, 'learning_rate': 4.6326757369614516e-05, 'epoch': 7.35}
{'loss': 0.1272, 'grad_norm': 0.20953132212162018, 'learning_rate': 4.630408163265306e-05, 'epoch': 7.39}
{'loss': 0.1266, 'grad_norm': 0.1855272650718689, 'learning_rate': 4.628140589569161e-05, 'epoch': 7.44}
{'loss': 0.1295, 'grad_norm': 0.1796029508113861, 'learning_rate': 4.625873015873016e-05, 'epoch': 7.48}
{'loss': 0.1236, 'grad_norm': 0.19807036221027374, 'learning_rate': 4.623605442176871e-05, 'epoch': 7.53}
{'loss': 0.1245, 'grad_norm': 0.21881982684135437, 'learning_rate': 4.621337868480726e-05, 'epoch': 7.57}
{'loss': 0.1275, 'grad_norm': 0.23750999569892883, 'learning_rate': 4.619070294784581e-05, 'epoch': 7.62}
{'loss': 0.124, 'grad_norm': 0.20224715769290924, 'learning_rate': 4.616802721088436e-05, 'epoch': 7.66}
{'loss': 0.1242, 'grad_norm': 0.30112597346305847, 'learning_rate': 4.61453514739229e-05, 'epoch': 7.71}
{'loss': 0.1268, 'grad_norm': 0.22862568497657776, 'learning_rate': 4.612267573696145e-05, 'epoch': 7.76}
{'loss': 0.124, 'grad_norm': 0.24928225576877594, 'learning_rate': 4.61e-05, 'epoch': 7.8}
{'loss': 0.1263, 'grad_norm': 0.1952267438173294, 'learning_rate': 4.607732426303855e-05, 'epoch': 7.85}
{'loss': 0.1253, 'grad_norm': 0.2103160321712494, 'learning_rate': 4.6054648526077096e-05, 'epoch': 7.89}
{'loss': 0.1247, 'grad_norm': 0.1848166584968567, 'learning_rate': 4.6031972789115646e-05, 'epoch': 7.94}
{'loss': 0.1253, 'grad_norm': 0.2087259739637375, 'learning_rate': 4.6009297052154196e-05, 'epoch': 7.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  8%|███▌                                        | 17640/220500 [31:06<3:13:53, 17.44it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4831159710884094, 'eval_runtime': 96.1227, 'eval_samples_per_second': 157.601, 'eval_steps_per_second': 4.931, 'epoch': 8.0}
  9%|███▉                                        | 19843/220500 [33:17<3:14:53, 17.16it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1263, 'grad_norm': 0.20757891237735748, 'learning_rate': 4.5986621315192746e-05, 'epoch': 8.03}
{'loss': 0.126, 'grad_norm': 0.16043053567409515, 'learning_rate': 4.5963945578231296e-05, 'epoch': 8.07}
{'loss': 0.1245, 'grad_norm': 0.18644455075263977, 'learning_rate': 4.5941269841269845e-05, 'epoch': 8.12}
{'loss': 0.1251, 'grad_norm': 0.18534033000469208, 'learning_rate': 4.5918594104308395e-05, 'epoch': 8.16}
{'loss': 0.1252, 'grad_norm': 0.22732968628406525, 'learning_rate': 4.589591836734694e-05, 'epoch': 8.21}
{'loss': 0.1249, 'grad_norm': 0.17426913976669312, 'learning_rate': 4.587324263038549e-05, 'epoch': 8.25}
{'loss': 0.1234, 'grad_norm': 0.1805822104215622, 'learning_rate': 4.585056689342404e-05, 'epoch': 8.3}
{'loss': 0.1246, 'grad_norm': 0.20649021863937378, 'learning_rate': 4.582789115646259e-05, 'epoch': 8.34}
{'loss': 0.1235, 'grad_norm': 0.20409277081489563, 'learning_rate': 4.580521541950114e-05, 'epoch': 8.39}
{'loss': 0.1237, 'grad_norm': 0.22793594002723694, 'learning_rate': 4.578253968253968e-05, 'epoch': 8.44}
{'loss': 0.1247, 'grad_norm': 0.17832176387310028, 'learning_rate': 4.575986394557824e-05, 'epoch': 8.48}
{'loss': 0.1245, 'grad_norm': 0.2320471554994583, 'learning_rate': 4.573718820861678e-05, 'epoch': 8.53}
{'loss': 0.1246, 'grad_norm': 0.1591535061597824, 'learning_rate': 4.571451247165533e-05, 'epoch': 8.57}
{'loss': 0.1252, 'grad_norm': 0.17931492626667023, 'learning_rate': 4.569183673469388e-05, 'epoch': 8.62}
{'loss': 0.1235, 'grad_norm': 0.22167998552322388, 'learning_rate': 4.566916099773243e-05, 'epoch': 8.66}
{'loss': 0.1244, 'grad_norm': 0.206563800573349, 'learning_rate': 4.5646485260770975e-05, 'epoch': 8.71}
{'loss': 0.1221, 'grad_norm': 0.16776257753372192, 'learning_rate': 4.5623809523809525e-05, 'epoch': 8.75}
{'loss': 0.1256, 'grad_norm': 0.1848752647638321, 'learning_rate': 4.5601133786848075e-05, 'epoch': 8.8}
{'loss': 0.126, 'grad_norm': 0.21571671962738037, 'learning_rate': 4.5578458049886625e-05, 'epoch': 8.84}
{'loss': 0.1245, 'grad_norm': 0.1828763782978058, 'learning_rate': 4.5555782312925175e-05, 'epoch': 8.89}
{'loss': 0.1234, 'grad_norm': 0.14703255891799927, 'learning_rate': 4.553310657596372e-05, 'epoch': 8.93}
{'loss': 0.123, 'grad_norm': 0.2003099024295807, 'learning_rate': 4.5510430839002275e-05, 'epoch': 8.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
  9%|███▉                                        | 19845/220500 [34:50<3:14:53, 17.16it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4806569516658783, 'eval_runtime': 93.0537, 'eval_samples_per_second': 162.799, 'eval_steps_per_second': 5.094, 'epoch': 9.0}
 10%|████▍                                       | 22048/220500 [37:01<3:14:45, 16.98it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1267, 'grad_norm': 0.19847989082336426, 'learning_rate': 4.548775510204082e-05, 'epoch': 9.02}
{'loss': 0.1229, 'grad_norm': 0.16346287727355957, 'learning_rate': 4.546507936507937e-05, 'epoch': 9.07}
{'loss': 0.1243, 'grad_norm': 0.19502179324626923, 'learning_rate': 4.544240362811791e-05, 'epoch': 9.12}
{'loss': 0.1239, 'grad_norm': 0.20545873045921326, 'learning_rate': 4.541972789115647e-05, 'epoch': 9.16}
{'loss': 0.1236, 'grad_norm': 0.2005692571401596, 'learning_rate': 4.539705215419501e-05, 'epoch': 9.21}
{'loss': 0.1226, 'grad_norm': 0.15803439915180206, 'learning_rate': 4.537437641723356e-05, 'epoch': 9.25}
{'loss': 0.1247, 'grad_norm': 0.20607273280620575, 'learning_rate': 4.535170068027211e-05, 'epoch': 9.3}
{'loss': 0.1248, 'grad_norm': 0.1900453120470047, 'learning_rate': 4.532902494331066e-05, 'epoch': 9.34}
{'loss': 0.1267, 'grad_norm': 0.21032948791980743, 'learning_rate': 4.530634920634921e-05, 'epoch': 9.39}
{'loss': 0.1242, 'grad_norm': 0.18503618240356445, 'learning_rate': 4.5283673469387754e-05, 'epoch': 9.43}
{'loss': 0.1237, 'grad_norm': 0.20647290349006653, 'learning_rate': 4.526099773242631e-05, 'epoch': 9.48}
{'loss': 0.1235, 'grad_norm': 0.19739815592765808, 'learning_rate': 4.5238321995464854e-05, 'epoch': 9.52}
{'loss': 0.1244, 'grad_norm': 0.17722247540950775, 'learning_rate': 4.5215646258503404e-05, 'epoch': 9.57}
{'loss': 0.1237, 'grad_norm': 0.1825900375843048, 'learning_rate': 4.519297052154195e-05, 'epoch': 9.61}
{'loss': 0.1265, 'grad_norm': 0.2000233232975006, 'learning_rate': 4.5170294784580504e-05, 'epoch': 9.66}
{'loss': 0.1234, 'grad_norm': 0.18357421457767487, 'learning_rate': 4.514761904761905e-05, 'epoch': 9.71}
{'loss': 0.1262, 'grad_norm': 0.21574831008911133, 'learning_rate': 4.51249433106576e-05, 'epoch': 9.75}
{'loss': 0.1239, 'grad_norm': 0.16222114861011505, 'learning_rate': 4.510226757369615e-05, 'epoch': 9.8}
{'loss': 0.1241, 'grad_norm': 0.13415192067623138, 'learning_rate': 4.50795918367347e-05, 'epoch': 9.84}
{'loss': 0.1247, 'grad_norm': 0.15962372720241547, 'learning_rate': 4.505691609977325e-05, 'epoch': 9.89}
{'loss': 0.1252, 'grad_norm': 0.1819603443145752, 'learning_rate': 4.503424036281179e-05, 'epoch': 9.93}
{'loss': 0.1253, 'grad_norm': 0.23233459889888763, 'learning_rate': 4.501156462585035e-05, 'epoch': 9.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 10%|████▍                                       | 22050/220500 [38:34<3:14:45, 16.98it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4795626103878021, 'eval_runtime': 93.0178, 'eval_samples_per_second': 162.861, 'eval_steps_per_second': 5.096, 'epoch': 10.0}
 11%|████▊                                       | 24253/220500 [40:46<3:08:42, 17.33it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1244, 'grad_norm': 0.16722002625465393, 'learning_rate': 4.498888888888889e-05, 'epoch': 10.02}
{'loss': 0.1222, 'grad_norm': 0.1353769600391388, 'learning_rate': 4.496621315192744e-05, 'epoch': 10.07}
{'loss': 0.123, 'grad_norm': 0.1841132491827011, 'learning_rate': 4.494353741496598e-05, 'epoch': 10.11}
{'loss': 0.1245, 'grad_norm': 0.19259415566921234, 'learning_rate': 4.492086167800454e-05, 'epoch': 10.16}
{'loss': 0.1242, 'grad_norm': 0.16626007854938507, 'learning_rate': 4.489818594104308e-05, 'epoch': 10.2}
{'loss': 0.1209, 'grad_norm': 0.16538214683532715, 'learning_rate': 4.487551020408163e-05, 'epoch': 10.25}
{'loss': 0.1246, 'grad_norm': 0.1945759356021881, 'learning_rate': 4.485283446712018e-05, 'epoch': 10.29}
{'loss': 0.123, 'grad_norm': 0.16876161098480225, 'learning_rate': 4.483015873015873e-05, 'epoch': 10.34}
{'loss': 0.1232, 'grad_norm': 0.1775168627500534, 'learning_rate': 4.480748299319728e-05, 'epoch': 10.39}
{'loss': 0.1229, 'grad_norm': 0.1858460009098053, 'learning_rate': 4.4784807256235826e-05, 'epoch': 10.43}
{'loss': 0.1239, 'grad_norm': 0.18073445558547974, 'learning_rate': 4.476213151927438e-05, 'epoch': 10.48}
{'loss': 0.1237, 'grad_norm': 0.1735423356294632, 'learning_rate': 4.4739455782312926e-05, 'epoch': 10.52}
{'loss': 0.1251, 'grad_norm': 0.20013222098350525, 'learning_rate': 4.4716780045351476e-05, 'epoch': 10.57}
{'loss': 0.1245, 'grad_norm': 0.16533170640468597, 'learning_rate': 4.469410430839002e-05, 'epoch': 10.61}
{'loss': 0.1235, 'grad_norm': 0.16875024139881134, 'learning_rate': 4.4671428571428576e-05, 'epoch': 10.66}
{'loss': 0.1231, 'grad_norm': 0.15420377254486084, 'learning_rate': 4.464875283446712e-05, 'epoch': 10.7}
{'loss': 0.1242, 'grad_norm': 0.22429344058036804, 'learning_rate': 4.462607709750567e-05, 'epoch': 10.75}
{'loss': 0.1243, 'grad_norm': 0.20194417238235474, 'learning_rate': 4.460340136054422e-05, 'epoch': 10.79}
{'loss': 0.1253, 'grad_norm': 0.19292916357517242, 'learning_rate': 4.458072562358277e-05, 'epoch': 10.84}
{'loss': 0.1245, 'grad_norm': 0.18448778986930847, 'learning_rate': 4.455804988662132e-05, 'epoch': 10.88}
{'loss': 0.121, 'grad_norm': 0.1620974838733673, 'learning_rate': 4.453537414965986e-05, 'epoch': 10.93}
{'loss': 0.1249, 'grad_norm': 0.24620427191257477, 'learning_rate': 4.451269841269842e-05, 'epoch': 10.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 11%|████▊                                       | 24255/220500 [42:19<3:08:42, 17.33it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4773557484149933, 'eval_runtime': 92.6104, 'eval_samples_per_second': 163.578, 'eval_steps_per_second': 5.118, 'epoch': 11.0}
 12%|█████▎                                      | 26458/220500 [44:29<3:07:06, 17.28it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1233, 'grad_norm': 0.1776459664106369, 'learning_rate': 4.449002267573696e-05, 'epoch': 11.02}
{'loss': 0.124, 'grad_norm': 0.1566609889268875, 'learning_rate': 4.446734693877551e-05, 'epoch': 11.07}
{'loss': 0.124, 'grad_norm': 0.20231893658638, 'learning_rate': 4.444467120181406e-05, 'epoch': 11.11}
{'loss': 0.1236, 'grad_norm': 0.16837961971759796, 'learning_rate': 4.442199546485261e-05, 'epoch': 11.16}
{'loss': 0.1239, 'grad_norm': 0.22253823280334473, 'learning_rate': 4.439931972789116e-05, 'epoch': 11.2}
{'loss': 0.1218, 'grad_norm': 0.2213650792837143, 'learning_rate': 4.4376643990929705e-05, 'epoch': 11.25}
{'loss': 0.1226, 'grad_norm': 0.17968930304050446, 'learning_rate': 4.435396825396826e-05, 'epoch': 11.29}
{'loss': 0.1242, 'grad_norm': 0.19557586312294006, 'learning_rate': 4.4331292517006805e-05, 'epoch': 11.34}
{'loss': 0.1224, 'grad_norm': 0.1940889060497284, 'learning_rate': 4.4308616780045355e-05, 'epoch': 11.38}
{'loss': 0.1242, 'grad_norm': 0.20785000920295715, 'learning_rate': 4.42859410430839e-05, 'epoch': 11.43}
{'loss': 0.125, 'grad_norm': 0.15963411331176758, 'learning_rate': 4.4263265306122455e-05, 'epoch': 11.47}
{'loss': 0.1241, 'grad_norm': 0.18004906177520752, 'learning_rate': 4.4240589569161e-05, 'epoch': 11.52}
{'loss': 0.1227, 'grad_norm': 0.16580767929553986, 'learning_rate': 4.421791383219955e-05, 'epoch': 11.56}
{'loss': 0.1251, 'grad_norm': 0.16533978283405304, 'learning_rate': 4.41952380952381e-05, 'epoch': 11.61}
{'loss': 0.1226, 'grad_norm': 0.17406362295150757, 'learning_rate': 4.417256235827665e-05, 'epoch': 11.66}
{'loss': 0.1238, 'grad_norm': 0.17809125781059265, 'learning_rate': 4.41498866213152e-05, 'epoch': 11.7}
{'loss': 0.1236, 'grad_norm': 0.1772582232952118, 'learning_rate': 4.412721088435374e-05, 'epoch': 11.75}
{'loss': 0.1218, 'grad_norm': 0.1712454855442047, 'learning_rate': 4.410453514739229e-05, 'epoch': 11.79}
{'loss': 0.1226, 'grad_norm': 0.1749560534954071, 'learning_rate': 4.408185941043084e-05, 'epoch': 11.84}
{'loss': 0.1222, 'grad_norm': 0.1466621607542038, 'learning_rate': 4.405918367346939e-05, 'epoch': 11.88}
{'loss': 0.1208, 'grad_norm': 0.15034650266170502, 'learning_rate': 4.4036507936507934e-05, 'epoch': 11.93}
{'loss': 0.1223, 'grad_norm': 0.16260401904582977, 'learning_rate': 4.401383219954649e-05, 'epoch': 11.97}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 12%|█████▎                                      | 26460/220500 [46:02<3:07:06, 17.28it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.47843247652053833, 'eval_runtime': 92.7649, 'eval_samples_per_second': 163.305, 'eval_steps_per_second': 5.11, 'epoch': 12.0}
 13%|█████▋                                      | 28663/220500 [48:12<3:04:38, 17.32it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1211, 'grad_norm': 0.18047510087490082, 'learning_rate': 4.3991156462585034e-05, 'epoch': 12.02}
{'loss': 0.1235, 'grad_norm': 0.14644448459148407, 'learning_rate': 4.3968480725623584e-05, 'epoch': 12.06}
{'loss': 0.1238, 'grad_norm': 0.15967592597007751, 'learning_rate': 4.3945804988662134e-05, 'epoch': 12.11}
{'loss': 0.1228, 'grad_norm': 0.15806889533996582, 'learning_rate': 4.3923129251700684e-05, 'epoch': 12.15}
{'loss': 0.124, 'grad_norm': 0.17375187575817108, 'learning_rate': 4.3900453514739234e-05, 'epoch': 12.2}
{'loss': 0.1211, 'grad_norm': 0.21755178272724152, 'learning_rate': 4.387777777777778e-05, 'epoch': 12.24}
{'loss': 0.1228, 'grad_norm': 0.19458147883415222, 'learning_rate': 4.385510204081633e-05, 'epoch': 12.29}
{'loss': 0.1231, 'grad_norm': 0.20603598654270172, 'learning_rate': 4.383242630385488e-05, 'epoch': 12.34}
{'loss': 0.1244, 'grad_norm': 0.19149546325206757, 'learning_rate': 4.380975056689343e-05, 'epoch': 12.38}
{'loss': 0.123, 'grad_norm': 0.2324686348438263, 'learning_rate': 4.378707482993197e-05, 'epoch': 12.43}
{'loss': 0.1209, 'grad_norm': 0.16176734864711761, 'learning_rate': 4.376439909297053e-05, 'epoch': 12.47}
{'loss': 0.1221, 'grad_norm': 0.17658011615276337, 'learning_rate': 4.374172335600907e-05, 'epoch': 12.52}
{'loss': 0.1213, 'grad_norm': 0.15464752912521362, 'learning_rate': 4.371904761904762e-05, 'epoch': 12.56}
{'loss': 0.126, 'grad_norm': 0.1767353117465973, 'learning_rate': 4.369637188208617e-05, 'epoch': 12.61}
{'loss': 0.1228, 'grad_norm': 0.2190188765525818, 'learning_rate': 4.367369614512472e-05, 'epoch': 12.65}
{'loss': 0.1226, 'grad_norm': 0.15950559079647064, 'learning_rate': 4.365102040816327e-05, 'epoch': 12.7}
{'loss': 0.1227, 'grad_norm': 0.15568681061267853, 'learning_rate': 4.3628344671201814e-05, 'epoch': 12.74}
{'loss': 0.1228, 'grad_norm': 0.18285903334617615, 'learning_rate': 4.3605668934240363e-05, 'epoch': 12.79}
{'loss': 0.1217, 'grad_norm': 0.19897346198558807, 'learning_rate': 4.3582993197278913e-05, 'epoch': 12.83}
{'loss': 0.1217, 'grad_norm': 0.14038187265396118, 'learning_rate': 4.3560317460317463e-05, 'epoch': 12.88}
{'loss': 0.1215, 'grad_norm': 0.16752220690250397, 'learning_rate': 4.353764172335601e-05, 'epoch': 12.93}
{'loss': 0.1231, 'grad_norm': 0.1635735034942627, 'learning_rate': 4.351496598639456e-05, 'epoch': 12.97}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 13%|█████▋                                      | 28665/220500 [49:45<3:04:38, 17.32it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4748707711696625, 'eval_runtime': 92.8381, 'eval_samples_per_second': 163.177, 'eval_steps_per_second': 5.106, 'epoch': 13.0}
 14%|██████▏                                     | 30868/220500 [51:54<3:03:18, 17.24it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1225, 'grad_norm': 0.21206164360046387, 'learning_rate': 4.3492290249433107e-05, 'epoch': 13.02}
{'loss': 0.1233, 'grad_norm': 0.19277074933052063, 'learning_rate': 4.3469614512471657e-05, 'epoch': 13.06}
{'loss': 0.1249, 'grad_norm': 0.1863337606191635, 'learning_rate': 4.3446938775510207e-05, 'epoch': 13.11}
{'loss': 0.1205, 'grad_norm': 0.16722321510314941, 'learning_rate': 4.3424263038548756e-05, 'epoch': 13.15}
{'loss': 0.1224, 'grad_norm': 0.18346360325813293, 'learning_rate': 4.3401587301587306e-05, 'epoch': 13.2}
{'loss': 0.1223, 'grad_norm': 0.14573074877262115, 'learning_rate': 4.337891156462585e-05, 'epoch': 13.24}
{'loss': 0.1223, 'grad_norm': 0.18332655727863312, 'learning_rate': 4.33562358276644e-05, 'epoch': 13.29}
{'loss': 0.1219, 'grad_norm': 0.15363995730876923, 'learning_rate': 4.333356009070295e-05, 'epoch': 13.33}
{'loss': 0.1236, 'grad_norm': 0.1727849692106247, 'learning_rate': 4.33108843537415e-05, 'epoch': 13.38}
{'loss': 0.1213, 'grad_norm': 0.3147236406803131, 'learning_rate': 4.328820861678004e-05, 'epoch': 13.42}
{'loss': 0.1221, 'grad_norm': 0.1630365252494812, 'learning_rate': 4.32655328798186e-05, 'epoch': 13.47}
{'loss': 0.1219, 'grad_norm': 0.1823604255914688, 'learning_rate': 4.324285714285715e-05, 'epoch': 13.51}
{'loss': 0.1214, 'grad_norm': 0.14509174227714539, 'learning_rate': 4.322018140589569e-05, 'epoch': 13.56}
{'loss': 0.1231, 'grad_norm': 0.140876904129982, 'learning_rate': 4.319750566893424e-05, 'epoch': 13.61}
{'loss': 0.1225, 'grad_norm': 0.17720270156860352, 'learning_rate': 4.317482993197279e-05, 'epoch': 13.65}
{'loss': 0.123, 'grad_norm': 0.17501650750637054, 'learning_rate': 4.315215419501134e-05, 'epoch': 13.7}
{'loss': 0.1215, 'grad_norm': 0.18732723593711853, 'learning_rate': 4.3129478458049886e-05, 'epoch': 13.74}
{'loss': 0.1227, 'grad_norm': 0.15067251026630402, 'learning_rate': 4.3106802721088436e-05, 'epoch': 13.79}
{'loss': 0.1229, 'grad_norm': 0.18506616353988647, 'learning_rate': 4.3084126984126986e-05, 'epoch': 13.83}
{'loss': 0.1241, 'grad_norm': 0.18435321748256683, 'learning_rate': 4.3061451247165536e-05, 'epoch': 13.88}
{'loss': 0.122, 'grad_norm': 0.1337263286113739, 'learning_rate': 4.3038775510204086e-05, 'epoch': 13.92}
{'loss': 0.1212, 'grad_norm': 0.15297961235046387, 'learning_rate': 4.3016099773242636e-05, 'epoch': 13.97}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 14%|██████▏                                     | 30870/220500 [53:27<3:03:18, 17.24it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4731243848800659, 'eval_runtime': 92.4759, 'eval_samples_per_second': 163.816, 'eval_steps_per_second': 5.126, 'epoch': 14.0}
 15%|██████▌                                     | 33073/220500 [55:37<3:01:01, 17.26it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1232, 'grad_norm': 0.18149827420711517, 'learning_rate': 4.2993424036281186e-05, 'epoch': 14.01}
{'loss': 0.1202, 'grad_norm': 0.1700410395860672, 'learning_rate': 4.297074829931973e-05, 'epoch': 14.06}
{'loss': 0.1235, 'grad_norm': 0.16204452514648438, 'learning_rate': 4.294807256235828e-05, 'epoch': 14.1}
{'loss': 0.1243, 'grad_norm': 0.16057802736759186, 'learning_rate': 4.292539682539683e-05, 'epoch': 14.15}
{'loss': 0.1231, 'grad_norm': 0.15658238530158997, 'learning_rate': 4.290272108843538e-05, 'epoch': 14.2}
{'loss': 0.121, 'grad_norm': 0.14503878355026245, 'learning_rate': 4.288004535147392e-05, 'epoch': 14.24}
{'loss': 0.1223, 'grad_norm': 0.163701593875885, 'learning_rate': 4.285736961451247e-05, 'epoch': 14.29}
{'loss': 0.12, 'grad_norm': 0.15304139256477356, 'learning_rate': 4.283469387755102e-05, 'epoch': 14.33}
{'loss': 0.12, 'grad_norm': 0.1721823811531067, 'learning_rate': 4.281201814058957e-05, 'epoch': 14.38}
{'loss': 0.1207, 'grad_norm': 0.1783561259508133, 'learning_rate': 4.278934240362812e-05, 'epoch': 14.42}
{'loss': 0.1213, 'grad_norm': 0.1638067364692688, 'learning_rate': 4.2766666666666665e-05, 'epoch': 14.47}
{'loss': 0.1229, 'grad_norm': 0.18220242857933044, 'learning_rate': 4.274399092970522e-05, 'epoch': 14.51}
{'loss': 0.1235, 'grad_norm': 0.16855785250663757, 'learning_rate': 4.2721315192743765e-05, 'epoch': 14.56}
{'loss': 0.121, 'grad_norm': 0.14448213577270508, 'learning_rate': 4.2698639455782315e-05, 'epoch': 14.6}
{'loss': 0.121, 'grad_norm': 0.15227964520454407, 'learning_rate': 4.2675963718820865e-05, 'epoch': 14.65}
{'loss': 0.1234, 'grad_norm': 0.16877876222133636, 'learning_rate': 4.2653287981859415e-05, 'epoch': 14.69}
{'loss': 0.1214, 'grad_norm': 0.17292115092277527, 'learning_rate': 4.263061224489796e-05, 'epoch': 14.74}
{'loss': 0.1219, 'grad_norm': 0.16780707240104675, 'learning_rate': 4.260793650793651e-05, 'epoch': 14.78}
{'loss': 0.1235, 'grad_norm': 0.161177396774292, 'learning_rate': 4.258526077097506e-05, 'epoch': 14.83}
{'loss': 0.123, 'grad_norm': 0.16708198189735413, 'learning_rate': 4.256258503401361e-05, 'epoch': 14.88}
{'loss': 0.1222, 'grad_norm': 0.15742506086826324, 'learning_rate': 4.253990929705216e-05, 'epoch': 14.92}
{'loss': 0.1228, 'grad_norm': 0.15580256283283234, 'learning_rate': 4.25172335600907e-05, 'epoch': 14.97}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 15%|██████▌                                     | 33075/220500 [57:10<3:01:01, 17.26it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.47220247983932495, 'eval_runtime': 92.6353, 'eval_samples_per_second': 163.534, 'eval_steps_per_second': 5.117, 'epoch': 15.0}
 16%|███████                                     | 35278/220500 [59:21<3:00:37, 17.09it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1233, 'grad_norm': 0.15638218820095062, 'learning_rate': 4.249455782312926e-05, 'epoch': 15.01}
{'loss': 0.1204, 'grad_norm': 0.13920211791992188, 'learning_rate': 4.24718820861678e-05, 'epoch': 15.06}
{'loss': 0.1221, 'grad_norm': 0.16307677328586578, 'learning_rate': 4.244920634920635e-05, 'epoch': 15.1}
{'loss': 0.1214, 'grad_norm': 0.16463227570056915, 'learning_rate': 4.24265306122449e-05, 'epoch': 15.15}
{'loss': 0.123, 'grad_norm': 0.14581075310707092, 'learning_rate': 4.240385487528345e-05, 'epoch': 15.19}
{'loss': 0.1237, 'grad_norm': 0.15544946491718292, 'learning_rate': 4.2381179138321994e-05, 'epoch': 15.24}
{'loss': 0.1207, 'grad_norm': 0.14117319881916046, 'learning_rate': 4.2358503401360544e-05, 'epoch': 15.28}
{'loss': 0.1222, 'grad_norm': 0.19907709956169128, 'learning_rate': 4.2335827664399094e-05, 'epoch': 15.33}
{'loss': 0.1216, 'grad_norm': 0.15788762271404266, 'learning_rate': 4.2313151927437644e-05, 'epoch': 15.37}
{'loss': 0.1248, 'grad_norm': 0.16148801147937775, 'learning_rate': 4.2290476190476194e-05, 'epoch': 15.42}
{'loss': 0.1203, 'grad_norm': 0.1839989721775055, 'learning_rate': 4.226780045351474e-05, 'epoch': 15.46}
{'loss': 0.1211, 'grad_norm': 0.14981819689273834, 'learning_rate': 4.2245124716553294e-05, 'epoch': 15.51}
{'loss': 0.1204, 'grad_norm': 0.15609224140644073, 'learning_rate': 4.222244897959184e-05, 'epoch': 15.56}
{'loss': 0.1228, 'grad_norm': 0.21533598005771637, 'learning_rate': 4.219977324263039e-05, 'epoch': 15.6}
{'loss': 0.1207, 'grad_norm': 0.14977970719337463, 'learning_rate': 4.217709750566894e-05, 'epoch': 15.65}
{'loss': 0.1217, 'grad_norm': 0.21054281294345856, 'learning_rate': 4.215442176870749e-05, 'epoch': 15.69}
{'loss': 0.121, 'grad_norm': 0.1648138463497162, 'learning_rate': 4.213174603174603e-05, 'epoch': 15.74}
{'loss': 0.1197, 'grad_norm': 0.15758460760116577, 'learning_rate': 4.210907029478458e-05, 'epoch': 15.78}
{'loss': 0.1225, 'grad_norm': 0.24432308971881866, 'learning_rate': 4.208639455782313e-05, 'epoch': 15.83}
{'loss': 0.1219, 'grad_norm': 0.20161719620227814, 'learning_rate': 4.206371882086168e-05, 'epoch': 15.87}
{'loss': 0.1228, 'grad_norm': 0.14844870567321777, 'learning_rate': 4.204104308390023e-05, 'epoch': 15.92}
{'loss': 0.1229, 'grad_norm': 0.13536030054092407, 'learning_rate': 4.201836734693877e-05, 'epoch': 15.96}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 16%|██████▋                                   | 35280/220500 [1:00:54<3:00:37, 17.09it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.47106221318244934, 'eval_runtime': 93.0118, 'eval_samples_per_second': 162.872, 'eval_steps_per_second': 5.096, 'epoch': 16.0}
 17%|███████▏                                  | 37483/220500 [1:03:06<2:58:05, 17.13it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1213, 'grad_norm': 0.17618657648563385, 'learning_rate': 4.199569160997733e-05, 'epoch': 16.01}
{'loss': 0.1228, 'grad_norm': 0.15421538054943085, 'learning_rate': 4.197301587301587e-05, 'epoch': 16.05}
{'loss': 0.1189, 'grad_norm': 0.1390056163072586, 'learning_rate': 4.195034013605442e-05, 'epoch': 16.1}
{'loss': 0.1236, 'grad_norm': 0.15365849435329437, 'learning_rate': 4.192766439909297e-05, 'epoch': 16.15}
{'loss': 0.1211, 'grad_norm': 0.18642476201057434, 'learning_rate': 4.190498866213152e-05, 'epoch': 16.19}
{'loss': 0.1234, 'grad_norm': 0.16480667889118195, 'learning_rate': 4.188231292517007e-05, 'epoch': 16.24}
{'loss': 0.122, 'grad_norm': 0.13594093918800354, 'learning_rate': 4.1859637188208616e-05, 'epoch': 16.28}
{'loss': 0.1223, 'grad_norm': 0.14689180254936218, 'learning_rate': 4.183696145124717e-05, 'epoch': 16.33}
{'loss': 0.1222, 'grad_norm': 0.17245009541511536, 'learning_rate': 4.1814285714285716e-05, 'epoch': 16.37}
{'loss': 0.1191, 'grad_norm': 0.16569942235946655, 'learning_rate': 4.1791609977324266e-05, 'epoch': 16.42}
{'loss': 0.1203, 'grad_norm': 0.1932435929775238, 'learning_rate': 4.176893424036281e-05, 'epoch': 16.46}
{'loss': 0.1203, 'grad_norm': 0.13572876155376434, 'learning_rate': 4.1746258503401366e-05, 'epoch': 16.51}
{'loss': 0.123, 'grad_norm': 0.17539379000663757, 'learning_rate': 4.172358276643991e-05, 'epoch': 16.55}
{'loss': 0.1209, 'grad_norm': 0.16694240272045135, 'learning_rate': 4.170090702947846e-05, 'epoch': 16.6}
{'loss': 0.1226, 'grad_norm': 0.16441571712493896, 'learning_rate': 4.167823129251701e-05, 'epoch': 16.64}
{'loss': 0.122, 'grad_norm': 0.168068528175354, 'learning_rate': 4.165555555555556e-05, 'epoch': 16.69}
{'loss': 0.1228, 'grad_norm': 0.16195246577262878, 'learning_rate': 4.163287981859411e-05, 'epoch': 16.73}
{'loss': 0.12, 'grad_norm': 0.11342057585716248, 'learning_rate': 4.161020408163265e-05, 'epoch': 16.78}
{'loss': 0.1227, 'grad_norm': 0.1582309752702713, 'learning_rate': 4.158752834467121e-05, 'epoch': 16.83}
{'loss': 0.121, 'grad_norm': 0.16355951130390167, 'learning_rate': 4.156485260770975e-05, 'epoch': 16.87}
{'loss': 0.1189, 'grad_norm': 0.18303996324539185, 'learning_rate': 4.15421768707483e-05, 'epoch': 16.92}
{'loss': 0.1222, 'grad_norm': 0.1571497917175293, 'learning_rate': 4.1519501133786845e-05, 'epoch': 16.96}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 17%|███████▏                                  | 37485/220500 [1:04:39<2:58:05, 17.13it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4691349267959595, 'eval_runtime': 93.083, 'eval_samples_per_second': 162.747, 'eval_steps_per_second': 5.092, 'epoch': 17.0}
 18%|███████▌                                  | 39688/220500 [1:06:50<2:56:05, 17.11it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1222, 'grad_norm': 0.14891891181468964, 'learning_rate': 4.14968253968254e-05, 'epoch': 17.01}
{'loss': 0.1226, 'grad_norm': 0.23269253969192505, 'learning_rate': 4.1474149659863945e-05, 'epoch': 17.05}
{'loss': 0.122, 'grad_norm': 0.1368687003850937, 'learning_rate': 4.1451473922902495e-05, 'epoch': 17.1}
{'loss': 0.1223, 'grad_norm': 0.15165938436985016, 'learning_rate': 4.1428798185941045e-05, 'epoch': 17.14}
{'loss': 0.1198, 'grad_norm': 0.1676364690065384, 'learning_rate': 4.1406122448979595e-05, 'epoch': 17.19}
{'loss': 0.1199, 'grad_norm': 0.16046780347824097, 'learning_rate': 4.1383446712018145e-05, 'epoch': 17.23}
{'loss': 0.1226, 'grad_norm': 0.1812509298324585, 'learning_rate': 4.136077097505669e-05, 'epoch': 17.28}
{'loss': 0.12, 'grad_norm': 0.13906003534793854, 'learning_rate': 4.1338095238095245e-05, 'epoch': 17.32}
{'loss': 0.1206, 'grad_norm': 0.1816527247428894, 'learning_rate': 4.131541950113379e-05, 'epoch': 17.37}
{'loss': 0.1191, 'grad_norm': 0.17315168678760529, 'learning_rate': 4.129274376417234e-05, 'epoch': 17.41}
{'loss': 0.1191, 'grad_norm': 0.16270938515663147, 'learning_rate': 4.127006802721088e-05, 'epoch': 17.46}
{'loss': 0.1204, 'grad_norm': 0.13979001343250275, 'learning_rate': 4.124739229024944e-05, 'epoch': 17.51}
{'loss': 0.1226, 'grad_norm': 0.16470244526863098, 'learning_rate': 4.122471655328798e-05, 'epoch': 17.55}
{'loss': 0.1224, 'grad_norm': 0.19568440318107605, 'learning_rate': 4.120204081632653e-05, 'epoch': 17.6}
{'loss': 0.1209, 'grad_norm': 0.22080594301223755, 'learning_rate': 4.117936507936508e-05, 'epoch': 17.64}
{'loss': 0.1225, 'grad_norm': 0.17087897658348083, 'learning_rate': 4.115668934240363e-05, 'epoch': 17.69}
{'loss': 0.1209, 'grad_norm': 0.16939593851566315, 'learning_rate': 4.113401360544218e-05, 'epoch': 17.73}
{'loss': 0.1222, 'grad_norm': 0.1882428526878357, 'learning_rate': 4.1111337868480725e-05, 'epoch': 17.78}
{'loss': 0.1207, 'grad_norm': 0.146722212433815, 'learning_rate': 4.108866213151928e-05, 'epoch': 17.82}
{'loss': 0.1218, 'grad_norm': 0.18888461589813232, 'learning_rate': 4.1065986394557824e-05, 'epoch': 17.87}
{'loss': 0.1215, 'grad_norm': 0.14346641302108765, 'learning_rate': 4.1043310657596374e-05, 'epoch': 17.91}
{'loss': 0.1216, 'grad_norm': 0.15973171591758728, 'learning_rate': 4.102063492063492e-05, 'epoch': 17.96}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 18%|███████▌                                  | 39690/220500 [1:08:23<2:56:04, 17.11it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4682062864303589, 'eval_runtime': 93.2431, 'eval_samples_per_second': 162.468, 'eval_steps_per_second': 5.083, 'epoch': 18.0}
 19%|███████▉                                  | 41893/220500 [1:10:33<2:53:01, 17.20it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1195, 'grad_norm': 0.16003704071044922, 'learning_rate': 4.0997959183673474e-05, 'epoch': 18.0}
{'loss': 0.1225, 'grad_norm': 0.14559581875801086, 'learning_rate': 4.097528344671202e-05, 'epoch': 18.05}
{'loss': 0.1217, 'grad_norm': 0.13402052223682404, 'learning_rate': 4.095260770975057e-05, 'epoch': 18.1}
{'loss': 0.1208, 'grad_norm': 0.19420219957828522, 'learning_rate': 4.092993197278912e-05, 'epoch': 18.14}
{'loss': 0.1167, 'grad_norm': 0.15499933063983917, 'learning_rate': 4.090725623582767e-05, 'epoch': 18.19}
{'loss': 0.1201, 'grad_norm': 0.13893292844295502, 'learning_rate': 4.088458049886622e-05, 'epoch': 18.23}
{'loss': 0.1211, 'grad_norm': 0.16612352430820465, 'learning_rate': 4.086190476190476e-05, 'epoch': 18.28}
{'loss': 0.1224, 'grad_norm': 0.1796557903289795, 'learning_rate': 4.083922902494332e-05, 'epoch': 18.32}
{'loss': 0.1225, 'grad_norm': 0.1674429476261139, 'learning_rate': 4.081655328798186e-05, 'epoch': 18.37}
{'loss': 0.1201, 'grad_norm': 0.15542887151241302, 'learning_rate': 4.079387755102041e-05, 'epoch': 18.41}
{'loss': 0.1188, 'grad_norm': 0.14773336052894592, 'learning_rate': 4.0771201814058954e-05, 'epoch': 18.46}
{'loss': 0.1218, 'grad_norm': 0.17243927717208862, 'learning_rate': 4.074852607709751e-05, 'epoch': 18.5}
{'loss': 0.1206, 'grad_norm': 0.14316576719284058, 'learning_rate': 4.0725850340136054e-05, 'epoch': 18.55}
{'loss': 0.1191, 'grad_norm': 0.15041567385196686, 'learning_rate': 4.0703174603174604e-05, 'epoch': 18.59}
{'loss': 0.1199, 'grad_norm': 0.13044683635234833, 'learning_rate': 4.0680498866213154e-05, 'epoch': 18.64}
{'loss': 0.12, 'grad_norm': 0.1601913571357727, 'learning_rate': 4.0657823129251704e-05, 'epoch': 18.68}
{'loss': 0.1227, 'grad_norm': 0.15913747251033783, 'learning_rate': 4.0635147392290254e-05, 'epoch': 18.73}
{'loss': 0.119, 'grad_norm': 0.1408044397830963, 'learning_rate': 4.06124716553288e-05, 'epoch': 18.78}
{'loss': 0.1198, 'grad_norm': 0.19708366692066193, 'learning_rate': 4.0589795918367353e-05, 'epoch': 18.82}
{'loss': 0.1226, 'grad_norm': 0.18332691490650177, 'learning_rate': 4.05671201814059e-05, 'epoch': 18.87}
{'loss': 0.1228, 'grad_norm': 0.17081138491630554, 'learning_rate': 4.054444444444445e-05, 'epoch': 18.91}
{'loss': 0.121, 'grad_norm': 0.1409701704978943, 'learning_rate': 4.0521768707483e-05, 'epoch': 18.96}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 19%|███████▉                                  | 41895/220500 [1:12:06<2:53:01, 17.20it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46674036979675293, 'eval_runtime': 92.4814, 'eval_samples_per_second': 163.806, 'eval_steps_per_second': 5.125, 'epoch': 19.0}
 20%|████████▍                                 | 44100/220500 [1:14:17<2:52:14, 17.07it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1202, 'grad_norm': 0.16143277287483215, 'learning_rate': 4.0499092970521547e-05, 'epoch': 19.0}
{'loss': 0.1173, 'grad_norm': 0.16952505707740784, 'learning_rate': 4.0476417233560097e-05, 'epoch': 19.05}
{'loss': 0.1224, 'grad_norm': 0.21313631534576416, 'learning_rate': 4.045374149659864e-05, 'epoch': 19.09}
{'loss': 0.1216, 'grad_norm': 0.17358390986919403, 'learning_rate': 4.043106575963719e-05, 'epoch': 19.14}
{'loss': 0.1219, 'grad_norm': 0.16573494672775269, 'learning_rate': 4.040839002267574e-05, 'epoch': 19.18}
{'loss': 0.121, 'grad_norm': 0.1982957422733307, 'learning_rate': 4.038571428571429e-05, 'epoch': 19.23}
{'loss': 0.1203, 'grad_norm': 0.1405402421951294, 'learning_rate': 4.036303854875283e-05, 'epoch': 19.27}
{'loss': 0.1198, 'grad_norm': 0.151832714676857, 'learning_rate': 4.034036281179139e-05, 'epoch': 19.32}
{'loss': 0.1218, 'grad_norm': 0.16003906726837158, 'learning_rate': 4.031768707482993e-05, 'epoch': 19.37}
{'loss': 0.1195, 'grad_norm': 0.1466946005821228, 'learning_rate': 4.029501133786848e-05, 'epoch': 19.41}
{'loss': 0.123, 'grad_norm': 0.13900527358055115, 'learning_rate': 4.027233560090703e-05, 'epoch': 19.46}
{'loss': 0.1206, 'grad_norm': 0.16311058402061462, 'learning_rate': 4.024965986394558e-05, 'epoch': 19.5}
{'loss': 0.1215, 'grad_norm': 0.1816042959690094, 'learning_rate': 4.022698412698413e-05, 'epoch': 19.55}
{'loss': 0.1196, 'grad_norm': 0.16241611540317535, 'learning_rate': 4.0204308390022676e-05, 'epoch': 19.59}
{'loss': 0.1227, 'grad_norm': 0.16371364891529083, 'learning_rate': 4.0181632653061226e-05, 'epoch': 19.64}
{'loss': 0.1193, 'grad_norm': 0.13628844916820526, 'learning_rate': 4.0158956916099776e-05, 'epoch': 19.68}
{'loss': 0.1223, 'grad_norm': 0.16282697021961212, 'learning_rate': 4.0136281179138326e-05, 'epoch': 19.73}
{'loss': 0.1204, 'grad_norm': 0.15229864418506622, 'learning_rate': 4.011360544217687e-05, 'epoch': 19.77}
{'loss': 0.1196, 'grad_norm': 0.14518161118030548, 'learning_rate': 4.009092970521542e-05, 'epoch': 19.82}
{'loss': 0.1219, 'grad_norm': 0.19183622300624847, 'learning_rate': 4.006825396825397e-05, 'epoch': 19.86}
{'loss': 0.1219, 'grad_norm': 0.15748564898967743, 'learning_rate': 4.004557823129252e-05, 'epoch': 19.91}
{'loss': 0.1194, 'grad_norm': 0.17566387355327606, 'learning_rate': 4.002290249433107e-05, 'epoch': 19.95}
{'loss': 0.1209, 'grad_norm': 0.28296390175819397, 'learning_rate': 4.000022675736962e-05, 'epoch': 20.0}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 20%|████████▍                                 | 44100/220500 [1:15:49<2:52:14, 17.07it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46580737829208374, 'eval_runtime': 92.4245, 'eval_samples_per_second': 163.907, 'eval_steps_per_second': 5.129, 'epoch': 20.0}
 21%|████████▊                                 | 46303/220500 [1:18:00<2:50:11, 17.06it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1207, 'grad_norm': 0.1634722352027893, 'learning_rate': 3.997755102040817e-05, 'epoch': 20.05}
{'loss': 0.1218, 'grad_norm': 0.15869022905826569, 'learning_rate': 3.995487528344671e-05, 'epoch': 20.09}
{'loss': 0.1212, 'grad_norm': 0.13682807981967926, 'learning_rate': 3.993219954648526e-05, 'epoch': 20.14}
{'loss': 0.1214, 'grad_norm': 0.15816456079483032, 'learning_rate': 3.990952380952381e-05, 'epoch': 20.18}
{'loss': 0.1218, 'grad_norm': 0.15222275257110596, 'learning_rate': 3.988684807256236e-05, 'epoch': 20.23}
{'loss': 0.1215, 'grad_norm': 0.14109355211257935, 'learning_rate': 3.9864172335600905e-05, 'epoch': 20.27}
{'loss': 0.1198, 'grad_norm': 0.1491725742816925, 'learning_rate': 3.9841496598639455e-05, 'epoch': 20.32}
{'loss': 0.1222, 'grad_norm': 0.20693297684192657, 'learning_rate': 3.9818820861678005e-05, 'epoch': 20.36}
{'loss': 0.1201, 'grad_norm': 0.15542414784431458, 'learning_rate': 3.9796145124716555e-05, 'epoch': 20.41}
{'loss': 0.1215, 'grad_norm': 0.17191962897777557, 'learning_rate': 3.9773469387755105e-05, 'epoch': 20.45}
{'loss': 0.1202, 'grad_norm': 0.1551537811756134, 'learning_rate': 3.9750793650793655e-05, 'epoch': 20.5}
{'loss': 0.1195, 'grad_norm': 0.1796349585056305, 'learning_rate': 3.9728117913832205e-05, 'epoch': 20.54}
{'loss': 0.1195, 'grad_norm': 0.16354013979434967, 'learning_rate': 3.970544217687075e-05, 'epoch': 20.59}
{'loss': 0.1212, 'grad_norm': 0.17568595707416534, 'learning_rate': 3.96827664399093e-05, 'epoch': 20.63}
{'loss': 0.1222, 'grad_norm': 0.23090974986553192, 'learning_rate': 3.966009070294785e-05, 'epoch': 20.68}
{'loss': 0.1202, 'grad_norm': 0.19867050647735596, 'learning_rate': 3.96374149659864e-05, 'epoch': 20.73}
{'loss': 0.1226, 'grad_norm': 0.1528749316930771, 'learning_rate': 3.961473922902494e-05, 'epoch': 20.77}
{'loss': 0.1207, 'grad_norm': 0.12133021652698517, 'learning_rate': 3.959206349206349e-05, 'epoch': 20.82}
{'loss': 0.1198, 'grad_norm': 0.13367024064064026, 'learning_rate': 3.956938775510204e-05, 'epoch': 20.86}
{'loss': 0.1207, 'grad_norm': 0.14716672897338867, 'learning_rate': 3.954671201814059e-05, 'epoch': 20.91}
{'loss': 0.1196, 'grad_norm': 0.1417480707168579, 'learning_rate': 3.952403628117914e-05, 'epoch': 20.95}
{'loss': 0.1206, 'grad_norm': 0.15881234407424927, 'learning_rate': 3.950136054421769e-05, 'epoch': 21.0}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 21%|████████▊                                 | 46305/220500 [1:19:33<2:50:11, 17.06it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4669657051563263, 'eval_runtime': 92.4823, 'eval_samples_per_second': 163.804, 'eval_steps_per_second': 5.125, 'epoch': 21.0}
 22%|█████████▏                                | 48508/220500 [1:21:44<2:48:18, 17.03it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1219, 'grad_norm': 0.15771278738975525, 'learning_rate': 3.947868480725624e-05, 'epoch': 21.04}
{'loss': 0.1191, 'grad_norm': 0.21409890055656433, 'learning_rate': 3.9456009070294784e-05, 'epoch': 21.09}
{'loss': 0.1213, 'grad_norm': 0.15706928074359894, 'learning_rate': 3.9433333333333334e-05, 'epoch': 21.13}
{'loss': 0.1205, 'grad_norm': 0.18035678565502167, 'learning_rate': 3.9410657596371884e-05, 'epoch': 21.18}
{'loss': 0.1199, 'grad_norm': 0.17753730714321136, 'learning_rate': 3.9387981859410434e-05, 'epoch': 21.22}
{'loss': 0.1204, 'grad_norm': 0.15345454216003418, 'learning_rate': 3.936530612244898e-05, 'epoch': 21.27}
{'loss': 0.1185, 'grad_norm': 0.19066086411476135, 'learning_rate': 3.934263038548753e-05, 'epoch': 21.32}
{'loss': 0.1207, 'grad_norm': 0.14440880715847015, 'learning_rate': 3.931995464852608e-05, 'epoch': 21.36}
{'loss': 0.1213, 'grad_norm': 0.14688418805599213, 'learning_rate': 3.929727891156463e-05, 'epoch': 21.41}
{'loss': 0.1209, 'grad_norm': 0.1956287920475006, 'learning_rate': 3.927460317460318e-05, 'epoch': 21.45}
{'loss': 0.1201, 'grad_norm': 0.1520419865846634, 'learning_rate': 3.925192743764173e-05, 'epoch': 21.5}
{'loss': 0.1216, 'grad_norm': 0.1464330106973648, 'learning_rate': 3.922925170068028e-05, 'epoch': 21.54}
{'loss': 0.1187, 'grad_norm': 0.14636804163455963, 'learning_rate': 3.920657596371882e-05, 'epoch': 21.59}
{'loss': 0.1202, 'grad_norm': 0.13584303855895996, 'learning_rate': 3.918390022675737e-05, 'epoch': 21.63}
{'loss': 0.1217, 'grad_norm': 0.1309511363506317, 'learning_rate': 3.916122448979592e-05, 'epoch': 21.68}
{'loss': 0.1209, 'grad_norm': 0.18865586817264557, 'learning_rate': 3.913854875283447e-05, 'epoch': 21.72}
{'loss': 0.1185, 'grad_norm': 0.14471478760242462, 'learning_rate': 3.911587301587302e-05, 'epoch': 21.77}
{'loss': 0.1198, 'grad_norm': 0.13935968279838562, 'learning_rate': 3.909319727891156e-05, 'epoch': 21.81}
{'loss': 0.1219, 'grad_norm': 0.175566628575325, 'learning_rate': 3.907052154195012e-05, 'epoch': 21.86}
{'loss': 0.1218, 'grad_norm': 0.24961617588996887, 'learning_rate': 3.904784580498866e-05, 'epoch': 21.9}
{'loss': 0.1214, 'grad_norm': 0.18419957160949707, 'learning_rate': 3.902517006802721e-05, 'epoch': 21.95}
{'loss': 0.1216, 'grad_norm': 0.16313187777996063, 'learning_rate': 3.900249433106576e-05, 'epoch': 22.0}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 22%|█████████▏                                | 48510/220500 [1:23:17<2:48:18, 17.03it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4647060036659241, 'eval_runtime': 92.6257, 'eval_samples_per_second': 163.551, 'eval_steps_per_second': 5.117, 'epoch': 22.0}
 23%|█████████▋                                | 50715/220500 [1:25:28<2:40:47, 17.60it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1201, 'grad_norm': 0.18095174431800842, 'learning_rate': 3.897981859410431e-05, 'epoch': 22.04}
{'loss': 0.1205, 'grad_norm': 0.13722290098667145, 'learning_rate': 3.8957142857142856e-05, 'epoch': 22.09}
{'loss': 0.1212, 'grad_norm': 0.1541365385055542, 'learning_rate': 3.8934467120181406e-05, 'epoch': 22.13}
{'loss': 0.12, 'grad_norm': 0.17528358101844788, 'learning_rate': 3.8911791383219956e-05, 'epoch': 22.18}
{'loss': 0.119, 'grad_norm': 0.1475059688091278, 'learning_rate': 3.8889115646258506e-05, 'epoch': 22.22}
{'loss': 0.1198, 'grad_norm': 0.20720858871936798, 'learning_rate': 3.8866439909297056e-05, 'epoch': 22.27}
{'loss': 0.1201, 'grad_norm': 0.14188893139362335, 'learning_rate': 3.88437641723356e-05, 'epoch': 22.31}
{'loss': 0.1202, 'grad_norm': 0.16334538161754608, 'learning_rate': 3.8821088435374156e-05, 'epoch': 22.36}
{'loss': 0.119, 'grad_norm': 0.14165857434272766, 'learning_rate': 3.87984126984127e-05, 'epoch': 22.4}
{'loss': 0.1213, 'grad_norm': 0.14728611707687378, 'learning_rate': 3.877573696145125e-05, 'epoch': 22.45}
{'loss': 0.1196, 'grad_norm': 0.14183412492275238, 'learning_rate': 3.875306122448979e-05, 'epoch': 22.49}
{'loss': 0.1199, 'grad_norm': 0.13031607866287231, 'learning_rate': 3.873038548752835e-05, 'epoch': 22.54}
{'loss': 0.1183, 'grad_norm': 0.13321469724178314, 'learning_rate': 3.870770975056689e-05, 'epoch': 22.59}
{'loss': 0.1203, 'grad_norm': 0.14060615003108978, 'learning_rate': 3.868503401360544e-05, 'epoch': 22.63}
{'loss': 0.1192, 'grad_norm': 0.13846208155155182, 'learning_rate': 3.866235827664399e-05, 'epoch': 22.68}
{'loss': 0.1225, 'grad_norm': 0.1441163718700409, 'learning_rate': 3.863968253968254e-05, 'epoch': 22.72}
{'loss': 0.1212, 'grad_norm': 0.13571856915950775, 'learning_rate': 3.861700680272109e-05, 'epoch': 22.77}
{'loss': 0.1208, 'grad_norm': 0.32943522930145264, 'learning_rate': 3.8594331065759636e-05, 'epoch': 22.81}
{'loss': 0.1184, 'grad_norm': 0.12049933522939682, 'learning_rate': 3.857165532879819e-05, 'epoch': 22.86}
{'loss': 0.1206, 'grad_norm': 0.14253588020801544, 'learning_rate': 3.8548979591836735e-05, 'epoch': 22.9}
{'loss': 0.12, 'grad_norm': 0.23447111248970032, 'learning_rate': 3.8526303854875285e-05, 'epoch': 22.95}
{'loss': 0.1191, 'grad_norm': 0.2302817851305008, 'learning_rate': 3.850362811791383e-05, 'epoch': 22.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 23%|█████████▋                                | 50715/220500 [1:27:01<2:40:47, 17.60it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46397748589515686, 'eval_runtime': 92.7156, 'eval_samples_per_second': 163.392, 'eval_steps_per_second': 5.112, 'epoch': 23.0}
 24%|██████████                                | 52918/220500 [1:29:12<2:44:07, 17.02it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1189, 'grad_norm': 0.13453418016433716, 'learning_rate': 3.8480952380952385e-05, 'epoch': 23.04}
{'loss': 0.1199, 'grad_norm': 0.15602993965148926, 'learning_rate': 3.845827664399093e-05, 'epoch': 23.08}
{'loss': 0.1186, 'grad_norm': 0.14435094594955444, 'learning_rate': 3.843560090702948e-05, 'epoch': 23.13}
{'loss': 0.1205, 'grad_norm': 0.13613827526569366, 'learning_rate': 3.841292517006803e-05, 'epoch': 23.17}
{'loss': 0.1204, 'grad_norm': 0.18072398006916046, 'learning_rate': 3.839024943310658e-05, 'epoch': 23.22}
{'loss': 0.1184, 'grad_norm': 0.1454014927148819, 'learning_rate': 3.836757369614513e-05, 'epoch': 23.27}
{'loss': 0.1193, 'grad_norm': 0.1688331812620163, 'learning_rate': 3.834489795918367e-05, 'epoch': 23.31}
{'loss': 0.1203, 'grad_norm': 0.15365329384803772, 'learning_rate': 3.832222222222223e-05, 'epoch': 23.36}
{'loss': 0.1213, 'grad_norm': 0.14923995733261108, 'learning_rate': 3.829954648526077e-05, 'epoch': 23.4}
{'loss': 0.1198, 'grad_norm': 0.13989335298538208, 'learning_rate': 3.827687074829932e-05, 'epoch': 23.45}
{'loss': 0.1198, 'grad_norm': 0.15094974637031555, 'learning_rate': 3.8254195011337865e-05, 'epoch': 23.49}
{'loss': 0.1212, 'grad_norm': 0.15279801189899445, 'learning_rate': 3.823151927437642e-05, 'epoch': 23.54}
{'loss': 0.1183, 'grad_norm': 0.12910859286785126, 'learning_rate': 3.8208843537414965e-05, 'epoch': 23.58}
{'loss': 0.119, 'grad_norm': 0.16195915639400482, 'learning_rate': 3.8186167800453515e-05, 'epoch': 23.63}
{'loss': 0.1212, 'grad_norm': 0.14354334771633148, 'learning_rate': 3.8163492063492065e-05, 'epoch': 23.67}
{'loss': 0.1201, 'grad_norm': 0.14804278314113617, 'learning_rate': 3.8140816326530615e-05, 'epoch': 23.72}
{'loss': 0.119, 'grad_norm': 0.13520701229572296, 'learning_rate': 3.8118140589569165e-05, 'epoch': 23.76}
{'loss': 0.1214, 'grad_norm': 0.14800968766212463, 'learning_rate': 3.809546485260771e-05, 'epoch': 23.81}
{'loss': 0.1182, 'grad_norm': 0.16710321605205536, 'learning_rate': 3.8072789115646264e-05, 'epoch': 23.85}
{'loss': 0.1179, 'grad_norm': 0.15898220241069794, 'learning_rate': 3.805011337868481e-05, 'epoch': 23.9}
{'loss': 0.1204, 'grad_norm': 0.15017184615135193, 'learning_rate': 3.802743764172336e-05, 'epoch': 23.95}
{'loss': 0.1216, 'grad_norm': 0.13672693073749542, 'learning_rate': 3.80047619047619e-05, 'epoch': 23.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 24%|██████████                                | 52920/220500 [1:30:45<2:44:07, 17.02it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.463163286447525, 'eval_runtime': 93.1986, 'eval_samples_per_second': 162.545, 'eval_steps_per_second': 5.086, 'epoch': 24.0}
 25%|██████████▌                               | 55125/220500 [1:32:57<2:36:44, 17.59it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1205, 'grad_norm': 0.15636059641838074, 'learning_rate': 3.798208616780046e-05, 'epoch': 24.04}
{'loss': 0.1198, 'grad_norm': 0.14934369921684265, 'learning_rate': 3.795941043083901e-05, 'epoch': 24.08}
{'loss': 0.1171, 'grad_norm': 0.14814279973506927, 'learning_rate': 3.793673469387755e-05, 'epoch': 24.13}
{'loss': 0.1201, 'grad_norm': 0.16724935173988342, 'learning_rate': 3.791405895691611e-05, 'epoch': 24.17}
{'loss': 0.1204, 'grad_norm': 0.20896951854228973, 'learning_rate': 3.789138321995465e-05, 'epoch': 24.22}
{'loss': 0.1205, 'grad_norm': 0.1429688185453415, 'learning_rate': 3.78687074829932e-05, 'epoch': 24.26}
{'loss': 0.1194, 'grad_norm': 0.17624002695083618, 'learning_rate': 3.7846031746031744e-05, 'epoch': 24.31}
{'loss': 0.1206, 'grad_norm': 0.12502071261405945, 'learning_rate': 3.78233560090703e-05, 'epoch': 24.35}
{'loss': 0.1184, 'grad_norm': 0.21561406552791595, 'learning_rate': 3.7800680272108844e-05, 'epoch': 24.4}
{'loss': 0.1186, 'grad_norm': 0.14028958976268768, 'learning_rate': 3.7778004535147394e-05, 'epoch': 24.44}
{'loss': 0.1206, 'grad_norm': 0.14644040167331696, 'learning_rate': 3.7755328798185944e-05, 'epoch': 24.49}
{'loss': 0.1201, 'grad_norm': 0.16612957417964935, 'learning_rate': 3.7732653061224494e-05, 'epoch': 24.54}
{'loss': 0.1181, 'grad_norm': 0.1557244211435318, 'learning_rate': 3.7709977324263044e-05, 'epoch': 24.58}
{'loss': 0.1195, 'grad_norm': 0.15088602900505066, 'learning_rate': 3.768730158730159e-05, 'epoch': 24.63}
{'loss': 0.1174, 'grad_norm': 0.16600851714611053, 'learning_rate': 3.7664625850340144e-05, 'epoch': 24.67}
{'loss': 0.12, 'grad_norm': 0.1276990920305252, 'learning_rate': 3.764195011337869e-05, 'epoch': 24.72}
{'loss': 0.1206, 'grad_norm': 0.22351767122745514, 'learning_rate': 3.761927437641724e-05, 'epoch': 24.76}
{'loss': 0.1187, 'grad_norm': 0.13784871995449066, 'learning_rate': 3.759659863945578e-05, 'epoch': 24.81}
{'loss': 0.1219, 'grad_norm': 0.14953039586544037, 'learning_rate': 3.757392290249434e-05, 'epoch': 24.85}
{'loss': 0.1183, 'grad_norm': 0.18867088854312897, 'learning_rate': 3.755124716553288e-05, 'epoch': 24.9}
{'loss': 0.1213, 'grad_norm': 0.1366732120513916, 'learning_rate': 3.752857142857143e-05, 'epoch': 24.94}
{'loss': 0.1207, 'grad_norm': 0.13684311509132385, 'learning_rate': 3.750589569160998e-05, 'epoch': 24.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 25%|██████████▌                               | 55125/220500 [1:34:30<2:36:44, 17.59it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46277040243148804, 'eval_runtime': 93.3233, 'eval_samples_per_second': 162.328, 'eval_steps_per_second': 5.079, 'epoch': 25.0}
 26%|██████████▉                               | 57328/220500 [1:36:42<2:40:00, 17.00it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1187, 'grad_norm': 0.13675221800804138, 'learning_rate': 3.748321995464853e-05, 'epoch': 25.03}
{'loss': 0.1204, 'grad_norm': 0.11851654946804047, 'learning_rate': 3.746054421768708e-05, 'epoch': 25.08}
{'loss': 0.118, 'grad_norm': 0.14937400817871094, 'learning_rate': 3.743786848072562e-05, 'epoch': 25.12}
{'loss': 0.1207, 'grad_norm': 0.14608092606067657, 'learning_rate': 3.741519274376417e-05, 'epoch': 25.17}
{'loss': 0.1191, 'grad_norm': 0.1727462112903595, 'learning_rate': 3.739251700680272e-05, 'epoch': 25.22}
{'loss': 0.119, 'grad_norm': 0.14915022253990173, 'learning_rate': 3.736984126984127e-05, 'epoch': 25.26}
{'loss': 0.1189, 'grad_norm': 0.16920435428619385, 'learning_rate': 3.7347165532879816e-05, 'epoch': 25.31}
{'loss': 0.1182, 'grad_norm': 0.15141154825687408, 'learning_rate': 3.732448979591837e-05, 'epoch': 25.35}
{'loss': 0.12, 'grad_norm': 0.13053852319717407, 'learning_rate': 3.7301814058956916e-05, 'epoch': 25.4}
{'loss': 0.1187, 'grad_norm': 0.13871356844902039, 'learning_rate': 3.7279138321995466e-05, 'epoch': 25.44}
{'loss': 0.1205, 'grad_norm': 0.17325015366077423, 'learning_rate': 3.7256462585034016e-05, 'epoch': 25.49}
{'loss': 0.1184, 'grad_norm': 0.1650681495666504, 'learning_rate': 3.7233786848072566e-05, 'epoch': 25.53}
{'loss': 0.1198, 'grad_norm': 0.1343541294336319, 'learning_rate': 3.7211111111111116e-05, 'epoch': 25.58}
{'loss': 0.1214, 'grad_norm': 0.16046157479286194, 'learning_rate': 3.718843537414966e-05, 'epoch': 25.62}
{'loss': 0.1213, 'grad_norm': 0.1404964178800583, 'learning_rate': 3.716575963718821e-05, 'epoch': 25.67}
{'loss': 0.1182, 'grad_norm': 0.1528734713792801, 'learning_rate': 3.714308390022676e-05, 'epoch': 25.71}
{'loss': 0.1193, 'grad_norm': 0.1412765383720398, 'learning_rate': 3.712040816326531e-05, 'epoch': 25.76}
{'loss': 0.1201, 'grad_norm': 0.15906156599521637, 'learning_rate': 3.709773242630385e-05, 'epoch': 25.8}
{'loss': 0.1207, 'grad_norm': 0.14093787968158722, 'learning_rate': 3.707505668934241e-05, 'epoch': 25.85}
{'loss': 0.12, 'grad_norm': 0.13450798392295837, 'learning_rate': 3.705238095238095e-05, 'epoch': 25.9}
{'loss': 0.1206, 'grad_norm': 0.14651669561862946, 'learning_rate': 3.70297052154195e-05, 'epoch': 25.94}
{'loss': 0.1204, 'grad_norm': 0.12793436646461487, 'learning_rate': 3.700702947845805e-05, 'epoch': 25.99}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 26%|██████████▉                               | 57330/220500 [1:38:16<2:40:00, 17.00it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46271947026252747, 'eval_runtime': 93.2367, 'eval_samples_per_second': 162.479, 'eval_steps_per_second': 5.084, 'epoch': 26.0}
 27%|███████████▎                              | 59533/220500 [1:40:28<2:37:53, 16.99it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1191, 'grad_norm': 0.1450100839138031, 'learning_rate': 3.69843537414966e-05, 'epoch': 26.03}
{'loss': 0.1187, 'grad_norm': 0.14761076867580414, 'learning_rate': 3.696167800453515e-05, 'epoch': 26.08}
{'loss': 0.1198, 'grad_norm': 0.13418935239315033, 'learning_rate': 3.6939002267573695e-05, 'epoch': 26.12}
{'loss': 0.1227, 'grad_norm': 0.1307758092880249, 'learning_rate': 3.6916326530612245e-05, 'epoch': 26.17}
{'loss': 0.1191, 'grad_norm': 0.12704825401306152, 'learning_rate': 3.6893650793650795e-05, 'epoch': 26.21}
{'loss': 0.1191, 'grad_norm': 0.1799090951681137, 'learning_rate': 3.6870975056689345e-05, 'epoch': 26.26}
{'loss': 0.1197, 'grad_norm': 0.12838882207870483, 'learning_rate': 3.684829931972789e-05, 'epoch': 26.3}
{'loss': 0.1185, 'grad_norm': 0.14058411121368408, 'learning_rate': 3.6825623582766445e-05, 'epoch': 26.35}
{'loss': 0.1188, 'grad_norm': 0.14406566321849823, 'learning_rate': 3.680294784580499e-05, 'epoch': 26.39}
{'loss': 0.1186, 'grad_norm': 0.16169637441635132, 'learning_rate': 3.678027210884354e-05, 'epoch': 26.44}
{'loss': 0.1212, 'grad_norm': 0.14958523213863373, 'learning_rate': 3.675759637188209e-05, 'epoch': 26.49}
{'loss': 0.12, 'grad_norm': 0.15826676785945892, 'learning_rate': 3.673492063492064e-05, 'epoch': 26.53}
{'loss': 0.1186, 'grad_norm': 0.17459192872047424, 'learning_rate': 3.671224489795919e-05, 'epoch': 26.58}
{'loss': 0.1183, 'grad_norm': 0.13070440292358398, 'learning_rate': 3.668956916099773e-05, 'epoch': 26.62}
{'loss': 0.1192, 'grad_norm': 0.14833682775497437, 'learning_rate': 3.666689342403628e-05, 'epoch': 26.67}
{'loss': 0.1188, 'grad_norm': 0.15276329219341278, 'learning_rate': 3.664421768707483e-05, 'epoch': 26.71}
{'loss': 0.1203, 'grad_norm': 0.13335448503494263, 'learning_rate': 3.662154195011338e-05, 'epoch': 26.76}
{'loss': 0.1195, 'grad_norm': 0.1383398324251175, 'learning_rate': 3.659886621315193e-05, 'epoch': 26.8}
{'loss': 0.1179, 'grad_norm': 0.12166661024093628, 'learning_rate': 3.657619047619048e-05, 'epoch': 26.85}
{'loss': 0.121, 'grad_norm': 0.16977813839912415, 'learning_rate': 3.655351473922903e-05, 'epoch': 26.89}
{'loss': 0.1201, 'grad_norm': 0.13094130158424377, 'learning_rate': 3.6530839002267574e-05, 'epoch': 26.94}
{'loss': 0.1208, 'grad_norm': 0.14303311705589294, 'learning_rate': 3.6508163265306124e-05, 'epoch': 26.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 27%|███████████▎                              | 59535/220500 [1:42:01<2:37:53, 16.99it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46172043681144714, 'eval_runtime': 93.0095, 'eval_samples_per_second': 162.876, 'eval_steps_per_second': 5.096, 'epoch': 27.0}
 28%|███████████▊                              | 61738/220500 [1:44:13<2:34:06, 17.17it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1179, 'grad_norm': 0.14719873666763306, 'learning_rate': 3.6485487528344674e-05, 'epoch': 27.03}
{'loss': 0.1171, 'grad_norm': 0.16312584280967712, 'learning_rate': 3.6462811791383224e-05, 'epoch': 27.07}
{'loss': 0.1178, 'grad_norm': 0.14740826189517975, 'learning_rate': 3.644013605442177e-05, 'epoch': 27.12}
{'loss': 0.1181, 'grad_norm': 0.13917821645736694, 'learning_rate': 3.641746031746032e-05, 'epoch': 27.17}
{'loss': 0.122, 'grad_norm': 0.12414208054542542, 'learning_rate': 3.639478458049887e-05, 'epoch': 27.21}
{'loss': 0.1205, 'grad_norm': 0.17161442339420319, 'learning_rate': 3.637210884353742e-05, 'epoch': 27.26}
{'loss': 0.12, 'grad_norm': 0.14722785353660583, 'learning_rate': 3.634943310657597e-05, 'epoch': 27.3}
{'loss': 0.1208, 'grad_norm': 0.14242279529571533, 'learning_rate': 3.632675736961452e-05, 'epoch': 27.35}
{'loss': 0.1208, 'grad_norm': 0.15472280979156494, 'learning_rate': 3.630408163265307e-05, 'epoch': 27.39}
{'loss': 0.1193, 'grad_norm': 0.16630569100379944, 'learning_rate': 3.628140589569161e-05, 'epoch': 27.44}
{'loss': 0.1185, 'grad_norm': 0.17263375222682953, 'learning_rate': 3.625873015873016e-05, 'epoch': 27.48}
{'loss': 0.1167, 'grad_norm': 0.18644775450229645, 'learning_rate': 3.623605442176871e-05, 'epoch': 27.53}
{'loss': 0.1198, 'grad_norm': 0.16136576235294342, 'learning_rate': 3.621337868480726e-05, 'epoch': 27.57}
{'loss': 0.1217, 'grad_norm': 0.15877823531627655, 'learning_rate': 3.6190702947845803e-05, 'epoch': 27.62}
{'loss': 0.1183, 'grad_norm': 0.14801275730133057, 'learning_rate': 3.6168027210884353e-05, 'epoch': 27.66}
{'loss': 0.1208, 'grad_norm': 0.15264588594436646, 'learning_rate': 3.6145351473922903e-05, 'epoch': 27.71}
{'loss': 0.1201, 'grad_norm': 0.13115529716014862, 'learning_rate': 3.612267573696145e-05, 'epoch': 27.76}
{'loss': 0.1189, 'grad_norm': 0.14461682736873627, 'learning_rate': 3.61e-05, 'epoch': 27.8}
{'loss': 0.1216, 'grad_norm': 0.14082635939121246, 'learning_rate': 3.6077324263038547e-05, 'epoch': 27.85}
{'loss': 0.1169, 'grad_norm': 0.14105691015720367, 'learning_rate': 3.60546485260771e-05, 'epoch': 27.89}
{'loss': 0.1185, 'grad_norm': 0.1239471286535263, 'learning_rate': 3.6031972789115646e-05, 'epoch': 27.94}
{'loss': 0.1179, 'grad_norm': 0.16963838040828705, 'learning_rate': 3.6009297052154196e-05, 'epoch': 27.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 28%|███████████▊                              | 61740/220500 [1:45:46<2:34:06, 17.17it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46105507016181946, 'eval_runtime': 93.1819, 'eval_samples_per_second': 162.574, 'eval_steps_per_second': 5.087, 'epoch': 28.0}
 29%|████████████▏                             | 63945/220500 [1:47:59<2:28:22, 17.58it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.119, 'grad_norm': 0.1316595822572708, 'learning_rate': 3.5986621315192746e-05, 'epoch': 28.03}
{'loss': 0.1198, 'grad_norm': 0.15669602155685425, 'learning_rate': 3.5963945578231296e-05, 'epoch': 28.07}
{'loss': 0.1183, 'grad_norm': 0.16267268359661102, 'learning_rate': 3.594126984126984e-05, 'epoch': 28.12}
{'loss': 0.116, 'grad_norm': 0.1423148661851883, 'learning_rate': 3.591859410430839e-05, 'epoch': 28.16}
{'loss': 0.1198, 'grad_norm': 0.1404649019241333, 'learning_rate': 3.589591836734694e-05, 'epoch': 28.21}
{'loss': 0.1186, 'grad_norm': 0.1392582207918167, 'learning_rate': 3.587324263038549e-05, 'epoch': 28.25}
{'loss': 0.1175, 'grad_norm': 0.1916239857673645, 'learning_rate': 3.585056689342404e-05, 'epoch': 28.3}
{'loss': 0.1185, 'grad_norm': 0.15193411707878113, 'learning_rate': 3.582789115646258e-05, 'epoch': 28.34}
{'loss': 0.1207, 'grad_norm': 0.12360908836126328, 'learning_rate': 3.580521541950114e-05, 'epoch': 28.39}
{'loss': 0.1201, 'grad_norm': 0.1469886749982834, 'learning_rate': 3.578253968253968e-05, 'epoch': 28.44}
{'loss': 0.1202, 'grad_norm': 0.1599777489900589, 'learning_rate': 3.575986394557823e-05, 'epoch': 28.48}
{'loss': 0.1193, 'grad_norm': 0.1442716419696808, 'learning_rate': 3.573718820861678e-05, 'epoch': 28.53}
{'loss': 0.1182, 'grad_norm': 0.1667616367340088, 'learning_rate': 3.571451247165533e-05, 'epoch': 28.57}
{'loss': 0.1186, 'grad_norm': 0.14705102145671844, 'learning_rate': 3.5691836734693876e-05, 'epoch': 28.62}
{'loss': 0.1192, 'grad_norm': 0.21551749110221863, 'learning_rate': 3.5669160997732426e-05, 'epoch': 28.66}
{'loss': 0.1195, 'grad_norm': 0.13041546940803528, 'learning_rate': 3.5646485260770976e-05, 'epoch': 28.71}
{'loss': 0.1181, 'grad_norm': 0.127677321434021, 'learning_rate': 3.5623809523809526e-05, 'epoch': 28.75}
{'loss': 0.1197, 'grad_norm': 0.15264558792114258, 'learning_rate': 3.5601133786848076e-05, 'epoch': 28.8}
{'loss': 0.1193, 'grad_norm': 0.14284968376159668, 'learning_rate': 3.557845804988662e-05, 'epoch': 28.84}
{'loss': 0.117, 'grad_norm': 0.21602670848369598, 'learning_rate': 3.5555782312925175e-05, 'epoch': 28.89}
{'loss': 0.1174, 'grad_norm': 0.12455720454454422, 'learning_rate': 3.553310657596372e-05, 'epoch': 28.93}
{'loss': 0.1194, 'grad_norm': 0.15138590335845947, 'learning_rate': 3.551043083900227e-05, 'epoch': 28.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 29%|████████████▏                             | 63945/220500 [1:49:34<2:28:22, 17.58it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46073275804519653, 'eval_runtime': 94.7295, 'eval_samples_per_second': 159.919, 'eval_steps_per_second': 5.004, 'epoch': 29.0}
 30%|████████████▌                             | 66148/220500 [1:51:46<2:32:16, 16.89it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1191, 'grad_norm': 0.14830195903778076, 'learning_rate': 3.548775510204082e-05, 'epoch': 29.02}
{'loss': 0.1196, 'grad_norm': 0.14950090646743774, 'learning_rate': 3.546507936507937e-05, 'epoch': 29.07}
{'loss': 0.1184, 'grad_norm': 0.18467073142528534, 'learning_rate': 3.544240362811791e-05, 'epoch': 29.12}
{'loss': 0.1192, 'grad_norm': 0.13537418842315674, 'learning_rate': 3.541972789115646e-05, 'epoch': 29.16}
{'loss': 0.12, 'grad_norm': 0.11803792417049408, 'learning_rate': 3.539705215419501e-05, 'epoch': 29.21}
{'loss': 0.1186, 'grad_norm': 0.133917436003685, 'learning_rate': 3.537437641723356e-05, 'epoch': 29.25}
{'loss': 0.1199, 'grad_norm': 0.13719122111797333, 'learning_rate': 3.535170068027211e-05, 'epoch': 29.3}
{'loss': 0.1196, 'grad_norm': 0.13142678141593933, 'learning_rate': 3.5329024943310655e-05, 'epoch': 29.34}
{'loss': 0.1194, 'grad_norm': 0.17741191387176514, 'learning_rate': 3.530634920634921e-05, 'epoch': 29.39}
{'loss': 0.1181, 'grad_norm': 0.1536875069141388, 'learning_rate': 3.5283673469387755e-05, 'epoch': 29.43}
{'loss': 0.1198, 'grad_norm': 0.1829177886247635, 'learning_rate': 3.5260997732426305e-05, 'epoch': 29.48}
{'loss': 0.1196, 'grad_norm': 0.16579000651836395, 'learning_rate': 3.5238321995464855e-05, 'epoch': 29.52}
{'loss': 0.1186, 'grad_norm': 0.15714918076992035, 'learning_rate': 3.5215646258503405e-05, 'epoch': 29.57}
{'loss': 0.119, 'grad_norm': 0.1677504926919937, 'learning_rate': 3.5192970521541955e-05, 'epoch': 29.61}
{'loss': 0.118, 'grad_norm': 0.15461629629135132, 'learning_rate': 3.51702947845805e-05, 'epoch': 29.66}
{'loss': 0.1235, 'grad_norm': 0.13828271627426147, 'learning_rate': 3.5147619047619055e-05, 'epoch': 29.71}
{'loss': 0.1196, 'grad_norm': 0.14706619083881378, 'learning_rate': 3.51249433106576e-05, 'epoch': 29.75}
{'loss': 0.1189, 'grad_norm': 0.12237321585416794, 'learning_rate': 3.510226757369615e-05, 'epoch': 29.8}
{'loss': 0.1196, 'grad_norm': 0.16718313097953796, 'learning_rate': 3.507959183673469e-05, 'epoch': 29.84}
{'loss': 0.1186, 'grad_norm': 0.14145436882972717, 'learning_rate': 3.505691609977325e-05, 'epoch': 29.89}
{'loss': 0.1187, 'grad_norm': 0.14099754393100739, 'learning_rate': 3.503424036281179e-05, 'epoch': 29.93}
{'loss': 0.1185, 'grad_norm': 0.11884485930204391, 'learning_rate': 3.501156462585034e-05, 'epoch': 29.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 30%|████████████▌                             | 66150/220500 [1:53:29<2:32:16, 16.89it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4600566625595093, 'eval_runtime': 102.9227, 'eval_samples_per_second': 147.188, 'eval_steps_per_second': 4.605, 'epoch': 30.0}
 31%|█████████████                             | 68353/220500 [1:55:42<2:32:33, 16.62it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1175, 'grad_norm': 0.14101919531822205, 'learning_rate': 3.498888888888889e-05, 'epoch': 30.02}
{'loss': 0.121, 'grad_norm': 0.16410690546035767, 'learning_rate': 3.496621315192744e-05, 'epoch': 30.07}
{'loss': 0.1184, 'grad_norm': 0.14775730669498444, 'learning_rate': 3.494353741496599e-05, 'epoch': 30.11}
{'loss': 0.1197, 'grad_norm': 0.14093105494976044, 'learning_rate': 3.4920861678004534e-05, 'epoch': 30.16}
{'loss': 0.1193, 'grad_norm': 0.1288040578365326, 'learning_rate': 3.489818594104309e-05, 'epoch': 30.2}
{'loss': 0.1192, 'grad_norm': 0.14297811686992645, 'learning_rate': 3.4875510204081634e-05, 'epoch': 30.25}
{'loss': 0.1181, 'grad_norm': 0.15073458850383759, 'learning_rate': 3.4852834467120184e-05, 'epoch': 30.29}
{'loss': 0.119, 'grad_norm': 0.15428486466407776, 'learning_rate': 3.483015873015873e-05, 'epoch': 30.34}
{'loss': 0.1187, 'grad_norm': 0.1324344426393509, 'learning_rate': 3.4807482993197284e-05, 'epoch': 30.39}
{'loss': 0.1191, 'grad_norm': 0.12647493183612823, 'learning_rate': 3.478480725623583e-05, 'epoch': 30.43}
{'loss': 0.1198, 'grad_norm': 0.20741169154644012, 'learning_rate': 3.476213151927438e-05, 'epoch': 30.48}
{'loss': 0.1177, 'grad_norm': 0.142439067363739, 'learning_rate': 3.473945578231293e-05, 'epoch': 30.52}
{'loss': 0.119, 'grad_norm': 0.1780381053686142, 'learning_rate': 3.471678004535148e-05, 'epoch': 30.57}
{'loss': 0.1195, 'grad_norm': 0.13258971273899078, 'learning_rate': 3.469410430839003e-05, 'epoch': 30.61}
{'loss': 0.1182, 'grad_norm': 0.13497550785541534, 'learning_rate': 3.467142857142857e-05, 'epoch': 30.66}
{'loss': 0.1196, 'grad_norm': 0.14916084706783295, 'learning_rate': 3.464875283446713e-05, 'epoch': 30.7}
{'loss': 0.1199, 'grad_norm': 0.14539629220962524, 'learning_rate': 3.462607709750567e-05, 'epoch': 30.75}
{'loss': 0.1189, 'grad_norm': 0.15423867106437683, 'learning_rate': 3.460340136054422e-05, 'epoch': 30.79}
{'loss': 0.1167, 'grad_norm': 0.14537902176380157, 'learning_rate': 3.458072562358276e-05, 'epoch': 30.84}
{'loss': 0.1207, 'grad_norm': 0.1426152139902115, 'learning_rate': 3.455804988662132e-05, 'epoch': 30.88}
{'loss': 0.1203, 'grad_norm': 0.13612258434295654, 'learning_rate': 3.453537414965986e-05, 'epoch': 30.93}
{'loss': 0.1189, 'grad_norm': 0.16415667533874512, 'learning_rate': 3.451269841269841e-05, 'epoch': 30.98}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 31%|█████████████                             | 68355/220500 [1:57:16<2:32:33, 16.62it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.46057823300361633, 'eval_runtime': 93.6919, 'eval_samples_per_second': 161.69, 'eval_steps_per_second': 5.059, 'epoch': 31.0}
 32%|█████████████▍                            | 70559/220500 [1:59:31<3:45:00, 11.11it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1182, 'grad_norm': 0.1379358321428299, 'learning_rate': 3.449002267573696e-05, 'epoch': 31.02}
{'loss': 0.1205, 'grad_norm': 0.147278293967247, 'learning_rate': 3.446734693877551e-05, 'epoch': 31.07}
{'loss': 0.1185, 'grad_norm': 0.17767144739627838, 'learning_rate': 3.444467120181406e-05, 'epoch': 31.11}
{'loss': 0.1203, 'grad_norm': 0.13589675724506378, 'learning_rate': 3.4421995464852606e-05, 'epoch': 31.16}
{'loss': 0.1183, 'grad_norm': 0.1407012641429901, 'learning_rate': 3.439931972789116e-05, 'epoch': 31.2}
{'loss': 0.1187, 'grad_norm': 0.14728718996047974, 'learning_rate': 3.4376643990929706e-05, 'epoch': 31.25}
{'loss': 0.1167, 'grad_norm': 0.13220924139022827, 'learning_rate': 3.4353968253968256e-05, 'epoch': 31.29}
{'loss': 0.12, 'grad_norm': 0.1404312402009964, 'learning_rate': 3.43312925170068e-05, 'epoch': 31.34}
{'loss': 0.1183, 'grad_norm': 0.14298364520072937, 'learning_rate': 3.4308616780045356e-05, 'epoch': 31.38}
{'loss': 0.1184, 'grad_norm': 0.16048727929592133, 'learning_rate': 3.42859410430839e-05, 'epoch': 31.43}
{'loss': 0.118, 'grad_norm': 0.15155214071273804, 'learning_rate': 3.426326530612245e-05, 'epoch': 31.47}
{'loss': 0.1176, 'grad_norm': 0.15937671065330505, 'learning_rate': 3.4240589569161e-05, 'epoch': 31.52}
{'loss': 0.1171, 'grad_norm': 0.1667582094669342, 'learning_rate': 3.421791383219955e-05, 'epoch': 31.56}
{'loss': 0.118, 'grad_norm': 0.13694709539413452, 'learning_rate': 3.41952380952381e-05, 'epoch': 31.61}
{'loss': 0.1203, 'grad_norm': 0.12820783257484436, 'learning_rate': 3.417256235827664e-05, 'epoch': 31.66}
{'loss': 0.1198, 'grad_norm': 0.1307477355003357, 'learning_rate': 3.41498866213152e-05, 'epoch': 31.7}
{'loss': 0.1185, 'grad_norm': 0.1268610656261444, 'learning_rate': 3.412721088435374e-05, 'epoch': 31.75}
{'loss': 0.1201, 'grad_norm': 0.16707755625247955, 'learning_rate': 3.410453514739229e-05, 'epoch': 31.79}
{'loss': 0.1181, 'grad_norm': 0.14010223746299744, 'learning_rate': 3.4081859410430835e-05, 'epoch': 31.84}
{'loss': 0.1185, 'grad_norm': 0.12756367027759552, 'learning_rate': 3.405918367346939e-05, 'epoch': 31.88}
{'loss': 0.1182, 'grad_norm': 0.16577021777629852, 'learning_rate': 3.4036507936507935e-05, 'epoch': 31.93}
{'loss': 0.1181, 'grad_norm': 0.2557191252708435, 'learning_rate': 3.4013832199546485e-05, 'epoch': 31.97}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 32%|█████████████▍                            | 70560/220500 [2:01:09<3:45:00, 11.11it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4605318307876587, 'eval_runtime': 97.926, 'eval_samples_per_second': 154.698, 'eval_steps_per_second': 4.84, 'epoch': 32.0}
 33%|█████████████▊                            | 72763/220500 [2:03:22<2:31:13, 16.28it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:37: UserWarning:
{'loss': 0.1176, 'grad_norm': 0.16944539546966553, 'learning_rate': 3.3991156462585035e-05, 'epoch': 32.02}
{'loss': 0.1212, 'grad_norm': 0.13654787838459015, 'learning_rate': 3.3968480725623585e-05, 'epoch': 32.06}
{'loss': 0.1186, 'grad_norm': 0.12059041857719421, 'learning_rate': 3.3945804988662135e-05, 'epoch': 32.11}
{'loss': 0.1175, 'grad_norm': 0.15325988829135895, 'learning_rate': 3.392312925170068e-05, 'epoch': 32.15}
{'loss': 0.1211, 'grad_norm': 0.11987610906362534, 'learning_rate': 3.3900453514739235e-05, 'epoch': 32.2}
{'loss': 0.1183, 'grad_norm': 0.1625663936138153, 'learning_rate': 3.387777777777778e-05, 'epoch': 32.24}
{'loss': 0.1169, 'grad_norm': 0.14037667214870453, 'learning_rate': 3.385510204081633e-05, 'epoch': 32.29}
{'loss': 0.1191, 'grad_norm': 0.13161928951740265, 'learning_rate': 3.383242630385488e-05, 'epoch': 32.34}
{'loss': 0.1172, 'grad_norm': 0.13653454184532166, 'learning_rate': 3.380975056689343e-05, 'epoch': 32.38}
{'loss': 0.1188, 'grad_norm': 0.14164166152477264, 'learning_rate': 3.378707482993198e-05, 'epoch': 32.43}
{'loss': 0.1182, 'grad_norm': 0.1289239525794983, 'learning_rate': 3.376439909297052e-05, 'epoch': 32.47}
{'loss': 0.1166, 'grad_norm': 0.16545961797237396, 'learning_rate': 3.374172335600907e-05, 'epoch': 32.52}
{'loss': 0.1189, 'grad_norm': 0.1404210925102234, 'learning_rate': 3.371904761904762e-05, 'epoch': 32.56}
{'loss': 0.1189, 'grad_norm': 0.1394975185394287, 'learning_rate': 3.369637188208617e-05, 'epoch': 32.61}
{'loss': 0.1167, 'grad_norm': 0.13475890457630157, 'learning_rate': 3.3673696145124714e-05, 'epoch': 32.65}
{'loss': 0.117, 'grad_norm': 0.16204288601875305, 'learning_rate': 3.365102040816327e-05, 'epoch': 32.7}
{'loss': 0.1213, 'grad_norm': 0.14483283460140228, 'learning_rate': 3.3628344671201814e-05, 'epoch': 32.74}
{'loss': 0.1197, 'grad_norm': 0.13018018007278442, 'learning_rate': 3.3605668934240364e-05, 'epoch': 32.79}
{'loss': 0.1202, 'grad_norm': 0.13315396010875702, 'learning_rate': 3.3582993197278914e-05, 'epoch': 32.83}
{'loss': 0.1188, 'grad_norm': 0.14250236749649048, 'learning_rate': 3.3560317460317464e-05, 'epoch': 32.88}
{'loss': 0.1157, 'grad_norm': 0.14847683906555176, 'learning_rate': 3.3537641723356014e-05, 'epoch': 32.93}
{'loss': 0.1195, 'grad_norm': 0.14763236045837402, 'learning_rate': 3.351496598639456e-05, 'epoch': 32.97}
    There is an imbalance between your GPUs. You may want to exclude GPU 3 which
    has less than 75% of the memory or cores of GPU 0. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(
/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
 33%|█████████████▊                            | 72765/220500 [2:04:57<2:31:13, 16.28it/s]/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(                                                                          
{'eval_loss': 0.4599849581718445, 'eval_runtime': 94.6238, 'eval_samples_per_second': 160.097, 'eval_steps_per_second': 5.009, 'epoch': 33.0}
 33%|█████████████▉                            | 73318/220500 [2:05:32<4:13:06,  9.69it/s]Traceback (most recent call last):
{'loss': 0.1169, 'grad_norm': 0.15810948610305786, 'learning_rate': 3.349229024943311e-05, 'epoch': 33.02}
{'loss': 0.1199, 'grad_norm': 0.12384716421365738, 'learning_rate': 3.346961451247166e-05, 'epoch': 33.06}
{'loss': 0.1184, 'grad_norm': 0.16532792150974274, 'learning_rate': 3.344693877551021e-05, 'epoch': 33.11}
{'loss': 0.1197, 'grad_norm': 0.1919969916343689, 'learning_rate': 3.342426303854875e-05, 'epoch': 33.15}
{'loss': 0.1184, 'grad_norm': 0.200693279504776, 'learning_rate': 3.34015873015873e-05, 'epoch': 33.2}
{'loss': 0.1179, 'grad_norm': 0.1539921760559082, 'learning_rate': 3.337891156462585e-05, 'epoch': 33.24}
  File "/home/augusto/symbo_repos/seringuela/scripts/train_test.py", line 533, in <module>
    main()
  File "/home/augusto/symbo_repos/seringuela/scripts/train_test.py", line 474, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/transformers/trainer.py", line 2236, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/augusto/symbo_repos/seringuela/.seriguela/lib/python3.11/site-packages/transformers/trainer.py", line 2559, in _inner_training_loop
    with context():
  File "/home/augusto/miniconda3/lib/python3.11/contextlib.py", line 757, in __exit__
    def __exit__(self, *excinfo):

KeyboardInterrupt
